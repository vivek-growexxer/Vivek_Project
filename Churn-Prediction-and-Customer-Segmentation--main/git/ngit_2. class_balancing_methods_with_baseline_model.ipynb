{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec16b495",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e4e014",
   "metadata": {},
   "source": [
    "### Root Directory and Time Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ec17ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creadint directory to store evaluation metrics for each methods\n",
    "evaluation_files_loc = '../files/class_balancing_methods_evaluations/'\n",
    "if not os.path.exists(evaluation_files_loc):\n",
    "    os.mkdir(evaluation_files_loc)\n",
    "    \n",
    "def time_logs(appr,method_name,tt):\n",
    "    with open(os.path.join(evaluation_files_loc,'time_logs'), 'a') as fp:\n",
    "        fp.writelines(f\"approach_{appr},{method_name},{tt}\" + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd69235",
   "metadata": {},
   "source": [
    "### Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3071f4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File location of the dataset\n",
    "data_loc = \"./Churn_Modelling.csv\"\n",
    "\n",
    "# Read the CSV file into a Pandas DataFrame, using the first column as the index\n",
    "df = pd.read_csv(data_loc, index_col=0)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6003f5",
   "metadata": {},
   "source": [
    "### Drop all categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ada866d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['CustomerId', 'Surname','Geography', 'Gender'], axis = 1,inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8bcf59",
   "metadata": {},
   "source": [
    "# Functions for model training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3e1b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_target_split(df, target_col='Exited'):\n",
    "    \"\"\"\n",
    "    Split the DataFrame into features and target variables.\n",
    "    \n",
    "    Parameters:\n",
    "        df (DataFrame): The input DataFrame.\n",
    "        target_col (str): The name of the target column. Default is 'Exited'.\n",
    "        \n",
    "    Returns:\n",
    "        x (DataFrame): The features.\n",
    "        y (Series): The target variable.\n",
    "    \"\"\"\n",
    "    # Drop the target column from the DataFrame to get the features\n",
    "    x = df.drop(target_col, axis=1)\n",
    "    \n",
    "    # Assign the target column as the y variable\n",
    "    y = df[target_col]\n",
    "    \n",
    "    # Return the features and target variables\n",
    "    return x,y\n",
    "\n",
    "def train_test_split(x,y,df,target_col='Exited', test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Split the features and target variables into training and testing sets.\n",
    "    \n",
    "    Parameters:\n",
    "        x (DataFrame): The features.\n",
    "        y (Series): The target variable.\n",
    "        df (DataFrame): The original DataFrame.\n",
    "        target_col (str): The name of the target column. Default is 'Exited'.\n",
    "        test_size (float or int): The proportion or absolute number of samples to include in the testing set. Default is 0.2.\n",
    "        random_state (int): The seed used by the random number generator. Default is 42.\n",
    "        \n",
    "    Returns:\n",
    "        x_train (DataFrame): The training set features.\n",
    "        x_test (DataFrame): The testing set features.\n",
    "        y_train (Series): The training set target variable.\n",
    "        y_test (Series): The testing set target variable.\n",
    "    \"\"\"\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    # Split the features and target variables into training and testing sets\n",
    "    # Stratified is being used to maintain the proportion of class [0 and 1] in splits.\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, \n",
    "                                                        test_size=test_size, \n",
    "                                                        random_state=random_state, \n",
    "                                                        stratify=df[target_col])\n",
    "    \n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "def logistic_model_train(x_train, y_train, random_state=42, max_iter=1000):\n",
    "    \"\"\"\n",
    "    Train a logistic regression model using the provided training data.\n",
    "    \n",
    "    Parameters:\n",
    "        x_train (DataFrame): The training set features.\n",
    "        y_train (Series): The training set target variable.\n",
    "        random_state (int): The seed used by the random number generator. Default is 42.\n",
    "        max_iter (int): The maximum number of iterations for the solver to converge. Default is 1000.\n",
    "        \n",
    "    Returns:\n",
    "        log_reg_model (LogisticRegression): The trained logistic regression model.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create an instance of LogisticRegression model with specified random_state and max_iter\n",
    "    log_reg_model = LogisticRegression(random_state=random_state, max_iter=max_iter)\n",
    "    \n",
    "    # Fit the logistic regression model to the training data\n",
    "    log_reg_model.fit(x_train, y_train)\n",
    "    \n",
    "    return log_reg_model\n",
    "\n",
    "def prediction(log_reg_model, x_train, x_test):\n",
    "    \"\"\"\n",
    "    Generate predictions using a trained logistic regression model.\n",
    "    \n",
    "    Parameters:\n",
    "        log_reg_model (LogisticRegression): The trained logistic regression model.\n",
    "        x_train (array-like or sparse matrix): The training set features.\n",
    "        x_test (array-like or sparse matrix): The testing set features.\n",
    "        \n",
    "    Returns:\n",
    "        y_pred_train (array-like): Predicted labels for the training set.\n",
    "        y_pred_test (array-like): Predicted labels for the testing set.\n",
    "        y_pred_test_proba (array-like): Predicted probabilities for the testing set.\n",
    "    \"\"\"\n",
    "    # Generate predictions for the training set\n",
    "    y_pred_train = log_reg_model.predict(x_train)\n",
    "    \n",
    "    # Generate predictions for the testing set\n",
    "    y_pred_test = log_reg_model.predict(x_test)\n",
    "    \n",
    "    # Generate predicted probabilities for the testing set\n",
    "    y_pred_test_proba = log_reg_model.predict_proba(x_test)\n",
    "    \n",
    "    return y_pred_train, y_pred_test, y_pred_test_proba\n",
    "\n",
    "class Evaluation():\n",
    "    def __init__(self,y_train, y_test, y_pred_train, y_pred_test, y_pred_test_proba):\n",
    "        self.y_train = y_train\n",
    "        self.y_test = y_test\n",
    "        self.y_pred_train = y_pred_train\n",
    "        self.y_pred_test = y_pred_test\n",
    "        self.y_pred_test_proba = y_pred_test_proba\n",
    "        \n",
    "    def __ks_stats_value__(self):\n",
    "        \"\"\"\n",
    "        Calculate the Kolmogorov-Smirnov (KS) statistic and p-value.\n",
    "        \n",
    "        Returns:\n",
    "            ks_stat (float): The KS statistic.\n",
    "            p_value (float): The p-value.\n",
    "        \"\"\"\n",
    "        \n",
    "        # proba_non_churn contains the predicted probabilities for instances that did not churn\n",
    "        proba_non_churn = self.y_pred_test_proba[:,1][self.y_test==0]\n",
    "        \n",
    "        # proba_churn contains the predicted probabilities for instances that actually churned\n",
    "        proba_churn = self.y_pred_test_proba[:,1][self.y_test==1]\n",
    "        \n",
    "        # Calculating Kolmogorov-Smirnov (KS) statistic and p-value\n",
    "        ks_stat, p_value = ks_2samp(proba_non_churn, proba_churn)\n",
    "        return ks_stat, p_value\n",
    "    \n",
    "    def __accuracy_value__(self):\n",
    "        train_accuracy = accuracy_score(self.y_train, self.y_pred_train)\n",
    "        test_accuracy = accuracy_score(self.y_test, self.y_pred_test)\n",
    "        return train_accuracy, test_accuracy\n",
    "\n",
    "    def __prec_rec_f1_value__(self, pos_label):\n",
    "        \"\"\"\n",
    "        Calculate precision, recall, and F1-score for a given label.\n",
    "        \n",
    "        Parameters:\n",
    "            pos_label: The label for which metrics are calculated.\n",
    "        \n",
    "        Returns:\n",
    "            precision (float): Precision score.\n",
    "            recall (float): Recall score.\n",
    "            f1 (float): F1-score.\n",
    "        \"\"\"\n",
    "        precision = precision_score(self.y_test, self.y_pred_test,pos_label=pos_label)\n",
    "        recall = recall_score(self.y_test, self.y_pred_test,pos_label=pos_label)\n",
    "        f1 = f1_score(self.y_test, self.y_pred_test, pos_label=pos_label)\n",
    "        return precision, recall, f1\n",
    "    \n",
    "    def __roc_value__(self):\n",
    "        roc_auc = roc_auc_score(self.y_test, self.y_pred_test)\n",
    "        return roc_auc\n",
    "    \n",
    "    def __confusion_matrix_value__(self):\n",
    "        tn, fp, fn, tp = confusion_matrix(self.y_test, self.y_pred_test).ravel()\n",
    "        return tn, fp, fn, tp\n",
    "    \n",
    "    def main(self):\n",
    "        train_accuracy, test_accuracy = self.__accuracy_value__()\n",
    "        \n",
    "        precision_0, recall_0, f1_0 = self.__prec_rec_f1_value__(pos_label=0)\n",
    "        precision_1, recall_1, f1_1 = self.__prec_rec_f1_value__(pos_label=1)\n",
    "        \n",
    "        ks_stat, p_value = self.__ks_stats_value__()\n",
    "        \n",
    "        roc_auc = self.__roc_value__()\n",
    "        \n",
    "        tn, fp, fn, tp = self.__confusion_matrix_value__()\n",
    "        \n",
    "        all_metrics = [train_accuracy, test_accuracy, roc_auc, \n",
    "                       precision_0, recall_0, f1_0, \n",
    "                       precision_1, recall_1, f1_1, \n",
    "                       ks_stat, p_value, \n",
    "                       tp, tn, fp, fn]\n",
    "        \n",
    "        all_metrics = [round(value, ndigits=6) for value in all_metrics]\n",
    "        all_metrics_dict = {'train_acc':all_metrics[0], 'test_acc':all_metrics[1], 'roc_auc':all_metrics[2],  \n",
    "                            'class_0':{'precision':all_metrics[3], 'recall':all_metrics[4], 'f1':all_metrics[5]}, \n",
    "                            'class_1':{'precision':all_metrics[6], 'recall':all_metrics[7], 'f1':all_metrics[8]},\n",
    "                            'ks_stats':all_metrics[9], 'p_value':all_metrics[10],\n",
    "                            'tp':all_metrics[11],'tn':all_metrics[12],'fp':all_metrics[13],'fn':all_metrics[14]}\n",
    "        \n",
    "        return all_metrics, all_metrics_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e642d1",
   "metadata": {},
   "source": [
    "# Class Balancing Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbae19c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smote_method(x,y,neighbour):\n",
    "    # Apply SMOTE\n",
    "    sm = SMOTE(random_state=42, k_neighbors=neighbour)\n",
    "    x_new, y_new = sm.fit_resample(x,y)\n",
    "    \n",
    "    return x_new, y_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e81f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adasyn_method(x,y,neighbour):\n",
    "    adap_synt = ADASYN(random_state=42, n_neighbors=neighbour)\n",
    "    x_new, y_new = adap_synt.fit_resample(x,y)\n",
    "    \n",
    "    return x_new, y_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15846a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def borderline_smote_method(x,y,neighbour):\n",
    "    border_smote = BorderlineSMOTE(random_state=42, k_neighbors=neighbour)\n",
    "    x_new, y_new = border_smote.fit_resample(x,y)\n",
    "\n",
    "    return x_new, y_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70e4986",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_smote_method(x,y,neighbour):\n",
    "    kmeans_smote = KMeansSMOTE(random_state=42, k_neighbors=neighbour,cluster_balance_threshold=0.2)\n",
    "    x_new, y_new = kmeans_smote.fit_resample(x,y)\n",
    "\n",
    "    return x_new, y_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e40c064",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smoten_method(x,y,neighbour):\n",
    "    # Apply SMOTEN\n",
    "    sm = SMOTEN(random_state=42, k_neighbors=neighbour)\n",
    "    x_new, y_new = sm.fit_resample(x,y)\n",
    "    \n",
    "    return x_new, y_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a41ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smotenc_method(x,y,neighbour, approach_type):\n",
    "    \n",
    "    # File location of the dataset\n",
    "    data_loc = \"./Churn_Modelling.csv\"\n",
    "\n",
    "    # Read the CSV file into a Pandas DataFrame, using the first column as the index\n",
    "    df = pd.read_csv(data_loc, index_col=0)\n",
    "    \n",
    "    df.drop(['CustomerId'], axis = 1,inplace=True)\n",
    "    \n",
    "    if approach_type == 1:\n",
    "    \n",
    "        x,y = features_target_split(df)\n",
    "\n",
    "        # Apply SMOTENC\n",
    "        sm = SMOTENC(categorical_features=[0,2,3],random_state=42, k_neighbors=neighbour)\n",
    "        x_new, y_new = sm.fit_resample(x,y)\n",
    "\n",
    "        x_new.drop(['Surname', 'Geography', 'Gender'], axis = 1,inplace=True)\n",
    "        \n",
    "        return x_new, y_new\n",
    "    \n",
    "    if approach_type == 2:\n",
    "        x,y = features_target_split(df)\n",
    "        \n",
    "        # Split the features and target variables into training and testing sets.\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x,y,df)\n",
    "\n",
    "        # Apply SMOTENC\n",
    "        sm = SMOTENC(categorical_features=[0,2,3],random_state=42, k_neighbors=neighbour)\n",
    "        x_train_new, y_train_new = sm.fit_resample(x_train,y_train)\n",
    "\n",
    "        x_train_new.drop(['Surname', 'Geography', 'Gender'], axis = 1,inplace=True)\n",
    "        \n",
    "        x_test.drop(['Surname', 'Geography', 'Gender'], axis = 1,inplace=True)\n",
    "        \n",
    "        return x_train_new, x_test, y_train_new, y_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31bf9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smote_enn_method(x,y,neighbour):\n",
    "    # Apply SMOTEENN\n",
    "    sm = SMOTEENN(random_state=42, smote=SMOTE(random_state=42, k_neighbors=neighbour))\n",
    "    x_new, y_new = sm.fit_resample(x,y)\n",
    "    \n",
    "    return x_new, y_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0cdf22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smote_tomek_method(x,y,neighbour):\n",
    "    # Apply SMOTETOMEK\n",
    "    sm = SMOTETomek(random_state=42, smote=SMOTE(random_state=42, k_neighbors=neighbour))\n",
    "    x_new, y_new = sm.fit_resample(x,y)\n",
    "    \n",
    "    return x_new, y_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2735cd12",
   "metadata": {},
   "source": [
    "# Training Baseline Model [Logistic Regression] \n",
    "\n",
    "### On raw data using numerical features only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfab162f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the DataFrame into features and target variables.\n",
    "x,y = features_target_split(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3490c43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the features and target variables into training and testing sets.\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y,df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c98f49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a logistic regression model\n",
    "log_reg_model = logistic_model_train(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021d59a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "y_pred_train, y_pred_test, y_pred_test_proba = prediction(log_reg_model, x_train, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e2a901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate evaluation metrics\n",
    "model_evaluation = Evaluation(y_train, y_test, y_pred_train, y_pred_test, y_pred_test_proba)\n",
    "_, eval_ = model_evaluation.main()\n",
    "eval_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f884dc1",
   "metadata": {},
   "source": [
    "# Using class balancing methods:\n",
    "\n",
    "### Two approaches tpo perform over sampling:\n",
    "\n",
    "**APPROACH 1** :: Over sampling is applied on the whole dataset i.e. before train, test split.\n",
    "\n",
    "**APPROACH 2** :: Over sampling is applied on the train dataset only i.e. after train, test split."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28838aeb",
   "metadata": {},
   "source": [
    "## Approach 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5549b2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def approach_one(df, method_name):\n",
    "    \n",
    "    print(f\"\\nUsing {method_name.upper()} :: APPROACH 1 :: \")\n",
    "    \n",
    "    metrics_data = []\n",
    "    \n",
    "    if method_name == 'kmeans_smote':\n",
    "        # No cluster are formed beyond this value of k\n",
    "        stop_range = 480\n",
    "        \n",
    "    elif method_name == 'smotenc':\n",
    "        # RAM issue\n",
    "        stop_range = 100\n",
    "        \n",
    "    else:\n",
    "        stop_range = df.Exited.value_counts()[1]\n",
    "\n",
    "    for neighbour in range(1,stop_range):\n",
    "\n",
    "        # Split the DataFrame into features and target variables.\n",
    "        x,y = features_target_split(df)\n",
    "\n",
    "        if method_name == 'smote':\n",
    "            # Apply SMOTE\n",
    "            x_new, y_new = smote_method(x,y,neighbour)\n",
    "\n",
    "        if method_name == 'adasyn':\n",
    "            # Apply ADASYN\n",
    "            x_new, y_new = adasyn_method(x,y,neighbour)\n",
    "            \n",
    "        if method_name == 'borderline_smote':\n",
    "            # Apply Borderline SMOTE\n",
    "            x_new, y_new = borderline_smote_method(x,y,neighbour)\n",
    "            \n",
    "        if method_name == 'kmeans_smote':\n",
    "            # Apply K-Means SMOTE\n",
    "            x_new, y_new = kmeans_smote_method(x,y,neighbour)\n",
    "            \n",
    "        if method_name == 'smoten':\n",
    "            # Apply SMOTEN\n",
    "            x_new, y_new = smoten_method(x,y,neighbour)\n",
    "            \n",
    "        if method_name == 'smotenc':\n",
    "            # Apply SMOTENC\n",
    "            x_new, y_new = smotenc_method(x,y,neighbour, approach_type=1)\n",
    "            \n",
    "        if method_name == 'svmsmote':\n",
    "            # Apply SVMSMOTE\n",
    "            x_new, y_new = svm_smote_method(x,y,neighbour)\n",
    "            \n",
    "        if method_name == 'smoteenn':\n",
    "            # Apply SMOTEENN\n",
    "            x_new, y_new = smote_enn_method(x,y,neighbour)\n",
    "            \n",
    "        if method_name == 'smotetomek':\n",
    "            # Apply SMOTETOMEK\n",
    "            x_new, y_new = smote_tomek_method(x,y,neighbour)\n",
    "\n",
    "        # Prepare new dataframe using generate data\n",
    "        df_new = pd.concat([x_new, y_new], axis=1)\n",
    "\n",
    "        # Split the DataFrame into features and target variables.\n",
    "        x,y = features_target_split(df_new)\n",
    "\n",
    "        # Split the features and target variables into training and testing sets.\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x,y,df_new)\n",
    "\n",
    "        # Train a logistic regression model\n",
    "        log_reg_model = logistic_model_train(x_train, y_train)\n",
    "\n",
    "        # Generate predictions\n",
    "        y_pred_train, y_pred_test, y_pred_test_proba = prediction(log_reg_model, x_train, x_test)\n",
    "\n",
    "        # Calculate evaluation metrics\n",
    "        model_evaluation = Evaluation(y_train, y_test, y_pred_train, y_pred_test, y_pred_test_proba)\n",
    "        all_metrics,_ = model_evaluation.main()\n",
    "\n",
    "        # inserting value of k in all_metrics\n",
    "        all_metrics.insert(0, neighbour)\n",
    "\n",
    "        metrics_data.append(all_metrics)\n",
    "        \n",
    "#         print(f\"\\n{neighbour}. Train Acc: {all_metrics[1]} :: Test Acc: {all_metrics[2]}\", end='')\n",
    "\n",
    "        if neighbour%500 == 0 or neighbour == 1:\n",
    "            print(f\"\\n{neighbour}. Train Acc: {all_metrics[1]} :: Test Acc: {all_metrics[2]}\",end='')\n",
    "        else:\n",
    "            if neighbour%10 == 0:\n",
    "                print('.', end='')\n",
    "    \n",
    "#         break\n",
    "    metrics_df = pd.DataFrame(data=metrics_data, columns=metrics_data_columns)\n",
    "    metrics_df.to_csv(os.path.join(evaluation_files_loc ,f\"./app_1_{method_name}_eval.csv\"), header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479893b5",
   "metadata": {},
   "source": [
    "## Approach 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f225c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def approach_two(df, method_name):\n",
    "    \n",
    "    print(f\"\\nUsing {method_name.upper()} :: APPROACH 2 :: \")\n",
    "    \n",
    "    metrics_data = []\n",
    "\n",
    "    # Split the DataFrame into features and target variables.\n",
    "    x,y = features_target_split(df)\n",
    "\n",
    "    # Split the features and target variables into training and testing sets.\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x,y,df)\n",
    "    \n",
    "    if method_name == 'kmeans_smote':\n",
    "        # No cluster are formed beyond this value of k\n",
    "        stop_range = 339\n",
    "        \n",
    "    elif method_name == 'smotenc':\n",
    "        # RAM issue\n",
    "        stop_range = 150\n",
    "        \n",
    "    else:\n",
    "        stop_range = y_train.value_counts()[1]\n",
    "\n",
    "\n",
    "    for neighbour in range(1,stop_range):\n",
    "        \n",
    "        if method_name == 'smote':\n",
    "            # Apply SMOTE\n",
    "            x_train_new, y_train_new = smote_method(x_train,y_train,neighbour)\n",
    "\n",
    "        if method_name == 'adasyn':\n",
    "            # Apply ADASYN\n",
    "            x_train_new, y_train_new = adasyn_method(x_train,y_train,neighbour)\n",
    "\n",
    "        if method_name == 'borderline_smote':\n",
    "            # Apply Borderline SMOTE\n",
    "            x_train_new, y_train_new = borderline_smote_method(x_train,y_train,neighbour)\n",
    "            \n",
    "        if method_name == 'kmeans_smote':\n",
    "            # Apply KMeans SMOTE\n",
    "            x_train_new, y_train_new = kmeans_smote_method(x_train,y_train,neighbour)\n",
    "            \n",
    "        if method_name == 'smoten':\n",
    "            # Apply SMOTEN\n",
    "            x_train_new, y_train_new = smoten_method(x_train,y_train,neighbour)\n",
    "            \n",
    "        if method_name == 'smotenc':\n",
    "            # Apply SMOTENC\n",
    "            x_train_new, x_test, y_train_new, y_test = smotenc_method(x_train,y_train,neighbour, approach_type=2)\n",
    "            \n",
    "        if method_name == 'svmsmote':\n",
    "            # Apply SVMSMOTE\n",
    "            x_train_new, y_train_new = svm_smote_method(x_train,y_train,neighbour)\n",
    "            \n",
    "        if method_name == 'smoteenn':\n",
    "            # Apply SMOTEENN\n",
    "            x_train_new, y_train_new = smote_enn_method(x_train,y_train,neighbour)\n",
    "            \n",
    "        if method_name == 'smotetomek':\n",
    "            # Apply SMOTETOMEK\n",
    "            x_train_new, y_train_new = smote_tomek_method(x_train,y_train,neighbour)\n",
    "            \n",
    "        # Train a logistic regression model\n",
    "        log_reg_model = logistic_model_train(x_train_new, y_train_new)\n",
    "\n",
    "        # Generate predictions\n",
    "        y_pred_train, y_pred_test, y_pred_test_proba = prediction(log_reg_model, x_train_new, x_test)\n",
    "\n",
    "        # Calculate evaluation metrics\n",
    "        model_evaluation = Evaluation(y_train_new, y_test, y_pred_train, y_pred_test, y_pred_test_proba)\n",
    "        all_metrics,_ = model_evaluation.main()\n",
    "\n",
    "        # inserting value of k in all_metrics\n",
    "        all_metrics.insert(0, neighbour)\n",
    "\n",
    "        metrics_data.append(all_metrics)\n",
    "        \n",
    "#         print(f\"\\n{neighbour}. Train Acc: {all_metrics[1]} :: Test Acc: {all_metrics[2]}\" , end='')\n",
    "\n",
    "        if neighbour%500 == 0 or neighbour == 1:\n",
    "            print(f\"\\n{neighbour}. Train Acc: {all_metrics[1]} :: Test Acc: {all_metrics[2]}\", end='')\n",
    "        else:\n",
    "            if neighbour%10 == 0:\n",
    "                print('.',end='')\n",
    "#         break\n",
    "    metrics_df = pd.DataFrame(data=metrics_data, columns=metrics_data_columns)\n",
    "    metrics_df.to_csv(os.path.join(evaluation_files_loc,f\"./app_2_{method_name}_eval.csv\"), header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9630e64",
   "metadata": {},
   "source": [
    "# Training Baseline Model\n",
    "\n",
    "### On raw data using numerical features only with class balancing methods using both approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6b8734",
   "metadata": {},
   "outputs": [],
   "source": [
    "methods_ = ['smote', 'adasyn', 'borderline_smote', \n",
    "            'kmeans_smote', 'smoten', 'smotenc' , \n",
    "            'svmsmote', 'smoteenn', 'smotetomek']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d495a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for mth in methods_:\n",
    "    print()\n",
    "    print('#'*50)\n",
    "    \n",
    "    start_time = time()\n",
    "    approach_one(df, method_name=mth)\n",
    "    end_time = time()\n",
    "    \n",
    "    print(\"\\nTime :: \", end_time-start_time)\n",
    "    time_logs('1',mth, round(end_time-start_time, ndigits=6))\n",
    "    \n",
    "    start_time = time()\n",
    "    approach_two(df, method_name=mth)\n",
    "    end_time = time()\n",
    "    \n",
    "    print(\"\\nTime :: \", end_time-start_time)\n",
    "    time_logs('2',mth,round(end_time-start_time, ndigits=6))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
