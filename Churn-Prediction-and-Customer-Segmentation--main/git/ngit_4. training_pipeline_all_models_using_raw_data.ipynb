{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02ff27a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
    "from imblearn.ensemble import BalancedBaggingClassifier, BalancedRandomForestClassifier, EasyEnsembleClassifier\n",
    "\n",
    "from scipy.stats import ks_2samp\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.metrics import f1_score, roc_auc_score, classification_report, confusion_matrix\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7558e2b0",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7880941",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = ['logistic_regression','gaussian_naive_bayes',\n",
    "              'support_vector_classifier','ada_boost',\n",
    "              'extra_trees_classifier','gradient_boosting_classifier',\n",
    "              'hist_gradient_boosting_classifier','random_forest_classifier',\n",
    "              'balanced_bagging_classifier','balanced_random_forest_classifier',\n",
    "              'easy_ensemble_classifier','lgbm_classifier','catboost_classifier']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f442efc9",
   "metadata": {},
   "source": [
    "### Metrics columns to store evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e0ca4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_metrics_data_columns = ['model', 'train_accuracy', 'test_accuracy', 'roc_auc',\n",
    "                               'precision_0', 'recall_0', 'f1_0',\n",
    "                               'precision_1', 'recall_1', 'f1_1', \n",
    "                               'ks_stat', 'p_value', \n",
    "                               'tp', 'tn', \n",
    "                               'fp', 'fn']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cd0c41",
   "metadata": {},
   "source": [
    "### Directory Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e16c2111",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_files_loc = '../files/baseline_models_evaluation/'\n",
    "if not os.path.exists(evaluation_files_loc):\n",
    "    os.makedirs(evaluation_files_loc)\n",
    "    \n",
    "# File location of the dataset\n",
    "data_loc = \"../files/Churn_Modelling.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58122db3",
   "metadata": {},
   "source": [
    "# Data Preperation and Evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8634133d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparation and Evaluation\n",
    "\n",
    "def features_target_split(df, target_col='Exited'):\n",
    "    \"\"\"\n",
    "    Split the DataFrame into features and target variables.\n",
    "    \n",
    "    Parameters:\n",
    "        df (DataFrame): The input DataFrame.\n",
    "        target_col (str): The name of the target column. Default is 'Exited'.\n",
    "        \n",
    "    Returns:\n",
    "        x (DataFrame): The features.\n",
    "        y (Series): The target variable.\n",
    "    \"\"\"\n",
    "    # Drop the target column from the DataFrame to get the features\n",
    "    x = df.drop(target_col, axis=1)\n",
    "    \n",
    "    # Assign the target column as the y variable\n",
    "    y = df[target_col]\n",
    "    \n",
    "    # Return the features and target variables\n",
    "    return x,y\n",
    "\n",
    "\n",
    "def train_test_split(x,y,df,target_col='Exited', test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Split the features and target variables into training and testing sets.\n",
    "    \n",
    "    Parameters:\n",
    "        x (DataFrame): The features.\n",
    "        y (Series): The target variable.\n",
    "        df (DataFrame): The original DataFrame.\n",
    "        target_col (str): The name of the target column. Default is 'Exited'.\n",
    "        test_size (float or int): The proportion or absolute number of samples to include in the testing set. Default is 0.2.\n",
    "        random_state (int): The seed used by the random number generator. Default is 42.\n",
    "        \n",
    "    Returns:\n",
    "        x_train (DataFrame): The training set features.\n",
    "        x_test (DataFrame): The testing set features.\n",
    "        y_train (Series): The training set target variable.\n",
    "        y_test (Series): The testing set target variable.\n",
    "    \"\"\"\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    # Split the features and target variables into training and testing sets\n",
    "    # Stratified is being used to maintain the proportion of class [0 and 1] in splits.\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, \n",
    "                                                        test_size=test_size, \n",
    "                                                        random_state=random_state, \n",
    "                                                        stratify=df[target_col])\n",
    "    \n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "\n",
    "def prediction(model, x_train, x_test):\n",
    "    \"\"\"\n",
    "    Generate predictions using a trained logistic regression model.\n",
    "    \n",
    "    Parameters:\n",
    "        log_reg_model (LogisticRegression): The trained logistic regression model.\n",
    "        x_train (array-like or sparse matrix): The training set features.\n",
    "        x_test (array-like or sparse matrix): The testing set features.\n",
    "        \n",
    "    Returns:\n",
    "        y_pred_train (array-like): Predicted labels for the training set.\n",
    "        y_pred_test (array-like): Predicted labels for the testing set.\n",
    "        y_pred_test_proba (array-like): Predicted probabilities for the testing set.\n",
    "    \"\"\"\n",
    "    # Generate predictions for the training set\n",
    "    y_pred_train = model.predict(x_train)\n",
    "    \n",
    "    # Generate predictions for the testing set\n",
    "    y_pred_test = model.predict(x_test)\n",
    "    \n",
    "    # Generate predicted probabilities for the testing set\n",
    "    y_pred_test_proba = model.predict_proba(x_test)\n",
    "    \n",
    "    return y_pred_train, y_pred_test, y_pred_test_proba\n",
    "\n",
    "\n",
    "class Evaluation():\n",
    "    def __init__(self,y_train, y_test, y_pred_train, y_pred_test, y_pred_test_proba):\n",
    "        self.y_train = y_train\n",
    "        self.y_test = y_test\n",
    "        self.y_pred_train = y_pred_train\n",
    "        self.y_pred_test = y_pred_test\n",
    "        self.y_pred_test_proba = y_pred_test_proba\n",
    "    \n",
    "    def __ks_stats_value__(self):\n",
    "        \"\"\"\n",
    "        Calculate the Kolmogorov-Smirnov (KS) statistic and p-value.\n",
    "        \n",
    "        Returns:\n",
    "            ks_stat (float): The KS statistic.\n",
    "            p_value (float): The p-value.\n",
    "        \"\"\"\n",
    "        \n",
    "        # proba_non_churn contains the predicted probabilities for instances that did not churn\n",
    "        proba_non_churn = self.y_pred_test_proba[:,1][self.y_test==0]\n",
    "        \n",
    "        # proba_churn contains the predicted probabilities for instances that actually churned\n",
    "        proba_churn = self.y_pred_test_proba[:,1][self.y_test==1]\n",
    "        \n",
    "        # Calculating Kolmogorov-Smirnov (KS) statistic and p-value\n",
    "        ks_stat, p_value = ks_2samp(proba_non_churn, proba_churn)\n",
    "        return ks_stat, p_value\n",
    "    \n",
    "    def __accuracy_value__(self):\n",
    "        train_accuracy = accuracy_score(self.y_train, self.y_pred_train)\n",
    "        test_accuracy = accuracy_score(self.y_test, self.y_pred_test)\n",
    "        return train_accuracy, test_accuracy\n",
    "\n",
    "    def __prec_rec_f1_value__(self, pos_label):\n",
    "        \"\"\"\n",
    "        Calculate precision, recall, and F1-score for a given label.\n",
    "        \n",
    "        Parameters:\n",
    "            pos_label: The label for which metrics are calculated.\n",
    "        \n",
    "        Returns:\n",
    "            precision (float): Precision score.\n",
    "            recall (float): Recall score.\n",
    "            f1 (float): F1-score.\n",
    "        \"\"\"\n",
    "        precision = precision_score(self.y_test, self.y_pred_test,pos_label=pos_label)\n",
    "        recall = recall_score(self.y_test, self.y_pred_test,pos_label=pos_label)\n",
    "        f1 = f1_score(self.y_test, self.y_pred_test, pos_label=pos_label)\n",
    "        return precision, recall, f1\n",
    "\n",
    "    def __roc_value__(self):\n",
    "        roc_auc = roc_auc_score(self.y_test, self.y_pred_test)\n",
    "        return roc_auc\n",
    "\n",
    "    def __confusion_matrix_value__(self):\n",
    "        tn, fp, fn, tp = confusion_matrix(self.y_test, self.y_pred_test).ravel()\n",
    "        return tn, fp, fn, tp\n",
    "    \n",
    "    def main(self):\n",
    "        train_accuracy, test_accuracy = self.__accuracy_value__()\n",
    "        \n",
    "        precision_0, recall_0, f1_0 = self.__prec_rec_f1_value__(pos_label=0)\n",
    "        precision_1, recall_1, f1_1 = self.__prec_rec_f1_value__(pos_label=1)\n",
    "        \n",
    "        ks_stat, p_value = self.__ks_stats_value__()\n",
    "        \n",
    "        roc_auc = self.__roc_value__()\n",
    "        \n",
    "        tn, fp, fn, tp = self.__confusion_matrix_value__()\n",
    "        \n",
    "        all_metrics = [train_accuracy, test_accuracy, roc_auc, \n",
    "                       precision_0, recall_0, f1_0, \n",
    "                       precision_1, recall_1, f1_1, \n",
    "                       ks_stat, p_value, \n",
    "                       tp, tn, fp, fn]\n",
    "        \n",
    "        all_metrics = [round(value, ndigits=6) for value in all_metrics]\n",
    "        all_metrics_dict = {'train_acc':all_metrics[0], 'test_acc':all_metrics[1], 'roc_auc':all_metrics[2],  \n",
    "                            'class_0':{'precision':all_metrics[3], 'recall':all_metrics[4], 'f1':all_metrics[5]}, \n",
    "                            'class_1':{'precision':all_metrics[6], 'recall':all_metrics[7], 'f1':all_metrics[8]},\n",
    "                            'ks_stats':all_metrics[9], 'p_value':all_metrics[10],\n",
    "                            'tp':all_metrics[11],'tn':all_metrics[12],'fp':all_metrics[13],'fn':all_metrics[14]}\n",
    "        \n",
    "        return all_metrics, all_metrics_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3941429",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "edfa2b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_model_train(x_train, y_train, random_state=42, max_iter=1000):\n",
    "    \"\"\"\n",
    "    Train a logistic regression model using the provided training data.\n",
    "    \n",
    "    Parameters:\n",
    "        x_train (DataFrame): The training set features.\n",
    "        y_train (Series): The training set target variable.\n",
    "        random_state (int): The seed used by the random number generator. Default is 42.\n",
    "        max_iter (int): The maximum number of iterations for the solver to converge. Default is 1000.\n",
    "        \n",
    "    Returns:\n",
    "        log_reg_model (LogisticRegression): The trained logistic regression model.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create an instance of LogisticRegression model with specified random_state and max_iter\n",
    "    log_reg_model = LogisticRegression(random_state=random_state, max_iter=max_iter)\n",
    "    \n",
    "    # Fit the logistic regression model to the training data\n",
    "    log_reg_model.fit(x_train, y_train)\n",
    "    \n",
    "    return log_reg_model\n",
    "\n",
    "\n",
    "def gnb_model_train(x_train, y_train):\n",
    "    \n",
    "    # instantiate the model\n",
    "    gnb = GaussianNB()\n",
    "    gnb.fit(x_train, y_train)\n",
    "    return gnb\n",
    "\n",
    "def svc_model_train(x_train, y_train, random_state=42):\n",
    "\n",
    "    svc = SVC(probability=True,random_state=random_state)\n",
    "    svc.fit(x_train, y_train)\n",
    "    return svc\n",
    "\n",
    "def adaboost_model_train(x_train, y_train, random_state=42):\n",
    "\n",
    "    adb_model = AdaBoostClassifier(random_state=random_state)\n",
    "    adb_model.fit(x_train, y_train)\n",
    "    return adb_model\n",
    "\n",
    "def etc_model_train(x_train, y_train, random_state=42):\n",
    "    etc_model = ExtraTreesClassifier(random_state=random_state)\n",
    "    etc_model.fit(x_train, y_train)\n",
    "    return etc_model\n",
    "\n",
    "def gbc_model_train(x_train, y_train, random_state=42):\n",
    "    gbc_model = GradientBoostingClassifier(random_state=random_state)\n",
    "    gbc_model.fit(x_train, y_train)\n",
    "    return gbc_model\n",
    "\n",
    "def hgbc_model_train(x_train, y_train, random_state=42):\n",
    "    hgbc_model = HistGradientBoostingClassifier(random_state=random_state)\n",
    "    hgbc_model.fit(x_train, y_train)\n",
    "    return hgbc_model\n",
    "\n",
    "def rfc_model_train(x_train, y_train, random_state=42):\n",
    "    rfc_model = RandomForestClassifier(random_state=random_state)\n",
    "    rfc_model.fit(x_train, y_train)\n",
    "    return rfc_model\n",
    "\n",
    "def bbc_model_train(x_train, y_train, random_state=42):\n",
    "    bbc_model = BalancedBaggingClassifier(random_state=random_state)\n",
    "    bbc_model.fit(x_train, y_train)\n",
    "    return bbc_model\n",
    "\n",
    "def brfc_model_train(x_train, y_train, random_state=42):\n",
    "    brfc_model = BalancedRandomForestClassifier(random_state=random_state)\n",
    "    brfc_model.fit(x_train, y_train)\n",
    "    return brfc_model\n",
    "\n",
    "def eec_model_train(x_train, y_train, random_state=42):\n",
    "    eec_model = EasyEnsembleClassifier(random_state=random_state)\n",
    "    eec_model.fit(x_train, y_train)\n",
    "    return eec_model\n",
    "\n",
    "def lgbm_model_train(x_train, y_train, random_state=42):\n",
    "    lgbm_model = LGBMClassifier(random_state=random_state)\n",
    "    lgbm_model.fit(x_train, y_train)\n",
    "    return lgbm_model\n",
    "\n",
    "def catboost_model_train(x_train, y_train, random_state=42):\n",
    "    catboost_model = CatBoostClassifier(random_state=random_state)\n",
    "    catboost_model.fit(x_train, y_train, verbose=False)\n",
    "    return catboost_model\n",
    "\n",
    "def train_all_models(x_train, y_train, model_name):\n",
    "    \n",
    "    if model_name == 'logistic_regression':\n",
    "        model = logistic_model_train(x_train, y_train)\n",
    "        \n",
    "    elif model_name == 'gaussian_naive_bayes':\n",
    "        model = gnb_model_train(x_train, y_train)\n",
    "        \n",
    "    elif model_name == 'support_vector_classifier':\n",
    "        model = svc_model_train(x_train, y_train)\n",
    "        \n",
    "    elif model_name == 'ada_boost':\n",
    "        model = adaboost_model_train(x_train, y_train)\n",
    "        \n",
    "    elif model_name == 'extra_trees_classifier':\n",
    "        model = etc_model_train(x_train, y_train)\n",
    "\n",
    "    elif model_name == 'gradient_boosting_classifier':\n",
    "        model = gbc_model_train(x_train, y_train)\n",
    "    \n",
    "    elif model_name == 'hist_gradient_boosting_classifier':\n",
    "        model = hgbc_model_train(x_train, y_train)\n",
    "    \n",
    "    elif model_name == 'random_forest_classifier':\n",
    "        model = rfc_model_train(x_train, y_train)\n",
    "\n",
    "    elif model_name == 'balanced_bagging_classifier':\n",
    "        model = bbc_model_train(x_train, y_train)\n",
    "        \n",
    "    elif model_name == 'balanced_random_forest_classifier':\n",
    "        model = brfc_model_train(x_train, y_train)\n",
    "        \n",
    "    elif model_name == 'easy_ensemble_classifier':\n",
    "        model = eec_model_train(x_train, y_train)\n",
    "\n",
    "    elif model_name == 'lgbm_classifier':\n",
    "        model = lgbm_model_train(x_train, y_train)\n",
    "\n",
    "    elif model_name == 'catboost_classifier':\n",
    "        model = catboost_model_train(x_train, y_train)\n",
    "\n",
    "    else:\n",
    "        print(\"Check model name\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ee3cb6",
   "metadata": {},
   "source": [
    "# Start Point"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50674dc2",
   "metadata": {},
   "source": [
    "Three approach is used to train all the models:\n",
    "\n",
    "**APPROACH 1**: Using numerical features only.\n",
    "\n",
    "**APPROACH 2**: Using categorical and numerical features. [Label Encoding is used]\n",
    "\n",
    "**APPROACH 3**: This is used only for CatBoost model as it supports to mark the columns as categorical."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09244005",
   "metadata": {},
   "source": [
    "## Approach 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e860d994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOGISTIC_REGRESSION                 :: Train Acc: 0.79625        :: Test Acc: 0.7965         :: F1_C1 :: 0.0\n",
      "GAUSSIAN_NAIVE_BAYES                :: Train Acc: 0.784375       :: Test Acc: 0.784          :: F1_C1 :: 0.096234\n",
      "SUPPORT_VECTOR_CLASSIFIER           :: Train Acc: 0.79625        :: Test Acc: 0.7965         :: F1_C1 :: 0.0\n",
      "ADA_BOOST                           :: Train Acc: 0.848375       :: Test Acc: 0.844          :: F1_C1 :: 0.542522\n",
      "EXTRA_TREES_CLASSIFIER              :: Train Acc: 1.0            :: Test Acc: 0.847          :: F1_C1 :: 0.514286\n",
      "GRADIENT_BOOSTING_CLASSIFIER        :: Train Acc: 0.86375        :: Test Acc: 0.8605         :: F1_C1 :: 0.566096\n",
      "HIST_GRADIENT_BOOSTING_CLASSIFIER   :: Train Acc: 0.900875       :: Test Acc: 0.8505         :: F1_C1 :: 0.543511\n",
      "RANDOM_FOREST_CLASSIFIER            :: Train Acc: 1.0            :: Test Acc: 0.8545         :: F1_C1 :: 0.544601\n",
      "BALANCED_BAGGING_CLASSIFIER         :: Train Acc: 0.93125        :: Test Acc: 0.795          :: F1_C1 :: 0.54646\n",
      "BALANCED_RANDOM_FOREST_CLASSIFIER   :: Train Acc: 0.89725        :: Test Acc: 0.7675         :: F1_C1 :: 0.559242\n",
      "EASY_ENSEMBLE_CLASSIFIER            :: Train Acc: 0.772          :: Test Acc: 0.77           :: F1_C1 :: 0.571695\n",
      "LGBM_CLASSIFIER                     :: Train Acc: 0.90525        :: Test Acc: 0.855          :: F1_C1 :: 0.559271\n",
      "CATBOOST_CLASSIFIER                 :: Train Acc: 0.896875       :: Test Acc: 0.859          :: F1_C1 :: 0.562112\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------------------------------------\n",
    "# Read the CSV file into a Pandas DataFrame, using the first column as the index\n",
    "df = pd.read_csv(data_loc, index_col=0)\n",
    "\n",
    "# Drop all categorical columns\n",
    "df.drop(['CustomerId', 'Surname','Geography', 'Gender'], axis = 1,inplace=True)\n",
    "df.head()\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Split the DataFrame into features and target variables.\n",
    "x,y = features_target_split(df)\n",
    "\n",
    "# Split the features and target variables into training and testing sets.\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y,df)\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Training\n",
    "actual_metrics_data = []\n",
    "for model_name in model_list:\n",
    "    model = train_all_models(x_train, y_train, model_name)\n",
    "    \n",
    "    # Generate predictions\n",
    "    y_pred_train, y_pred_test, y_pred_test_proba = prediction(model, x_train, x_test)\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    model_evaluation = Evaluation(y_train, y_test, y_pred_train, y_pred_test, y_pred_test_proba)\n",
    "    \n",
    "    all_metrics,_ = model_evaluation.main()\n",
    "    all_metrics.insert(0, model_name)\n",
    "    \n",
    "    print(\"{:<35} :: Train Acc: {:<14} :: Test Acc: {:<14} :: F1_C1 :: {}\".format(model_name.upper(), \n",
    "                                                                                  all_metrics[1], \n",
    "                                                                                  all_metrics[2], \n",
    "                                                                                  all_metrics[9]))\n",
    "\n",
    "    actual_metrics_data.append(all_metrics)\n",
    "#     break\n",
    "    \n",
    "metrics_df = pd.DataFrame(actual_metrics_data, columns=actual_metrics_data_columns)\n",
    "metrics_df.to_csv(os.path.join(evaluation_files_loc, 'baseline_models_eval_num_data.csv'), header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21eb3001",
   "metadata": {},
   "source": [
    "## Approach 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f4689b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOGISTIC_REGRESSION                 :: Train Acc: 0.7895         :: Test Acc: 0.7895         :: F1_C1 :: 0.086768\n",
      "GAUSSIAN_NAIVE_BAYES                :: Train Acc: 0.7845         :: Test Acc: 0.7865         :: F1_C1 :: 0.112266\n",
      "SUPPORT_VECTOR_CLASSIFIER           :: Train Acc: 0.79625        :: Test Acc: 0.7965         :: F1_C1 :: 0.0\n",
      "ADA_BOOST                           :: Train Acc: 0.857625       :: Test Acc: 0.8545         :: F1_C1 :: 0.568889\n",
      "EXTRA_TREES_CLASSIFIER              :: Train Acc: 1.0            :: Test Acc: 0.8595         :: F1_C1 :: 0.548957\n",
      "GRADIENT_BOOSTING_CLASSIFIER        :: Train Acc: 0.8715         :: Test Acc: 0.8695         :: F1_C1 :: 0.600306\n",
      "HIST_GRADIENT_BOOSTING_CLASSIFIER   :: Train Acc: 0.92225        :: Test Acc: 0.8605         :: F1_C1 :: 0.586667\n",
      "RANDOM_FOREST_CLASSIFIER            :: Train Acc: 1.0            :: Test Acc: 0.863          :: F1_C1 :: 0.565079\n",
      "BALANCED_BAGGING_CLASSIFIER         :: Train Acc: 0.931375       :: Test Acc: 0.806          :: F1_C1 :: 0.570796\n",
      "BALANCED_RANDOM_FOREST_CLASSIFIER   :: Train Acc: 0.9065         :: Test Acc: 0.7835         :: F1_C1 :: 0.594949\n",
      "EASY_ENSEMBLE_CLASSIFIER            :: Train Acc: 0.785125       :: Test Acc: 0.78           :: F1_C1 :: 0.588015\n",
      "LGBM_CLASSIFIER                     :: Train Acc: 0.920625       :: Test Acc: 0.8585         :: F1_C1 :: 0.576981\n",
      "CATBOOST_CLASSIFIER                 :: Train Acc: 0.914625       :: Test Acc: 0.8625         :: F1_C1 :: 0.582701\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Read the CSV file into a Pandas DataFrame, using the first column as the index\n",
    "df = pd.read_csv(data_loc, index_col=0)\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Converting type of columns to category\n",
    "df['Geography'] = df['Geography'].astype('category')\n",
    "df['Gender'] = df['Gender'].astype('category')\n",
    "df['Surname'] = df['Surname'].astype('category')\n",
    "\n",
    "# Assigning numerical values and storing it in another columns\n",
    "df['Geography_new'] = df['Geography'].cat.codes\n",
    "df['Gender_new'] = df['Gender'].cat.codes\n",
    "df['Surname_new'] = df['Surname'].cat.codes\n",
    "\n",
    "df['Geography'] = df['Geography_new']\n",
    "df['Gender'] = df['Gender_new']\n",
    "df['Surname'] = df['Surname_new']\n",
    "\n",
    "df.drop(['CustomerId','Surname_new','Geography_new', 'Gender_new'], axis = 1,inplace=True)\n",
    "# df.drop(['CustomerId', 'Surname','Geography', 'Gender'], axis = 1,inplace=True)\n",
    "df.head()\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Split the DataFrame into features and target variables.\n",
    "x,y = features_target_split(df)\n",
    "\n",
    "# Split the features and target variables into training and testing sets.\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y,df)\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Training\n",
    "\n",
    "actual_metrics_data = []\n",
    "for model_name in model_list:\n",
    "    model = train_all_models(x_train, y_train, model_name)\n",
    "    \n",
    "    # Generate predictions\n",
    "    y_pred_train, y_pred_test, y_pred_test_proba = prediction(model, x_train, x_test)\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    model_evaluation = Evaluation(y_train, y_test, y_pred_train, y_pred_test, y_pred_test_proba)\n",
    "    \n",
    "    all_metrics,_ = model_evaluation.main()\n",
    "    all_metrics.insert(0, model_name)\n",
    "    \n",
    "    print(\"{:<35} :: Train Acc: {:<14} :: Test Acc: {:<14} :: F1_C1 :: {}\".format(model_name.upper(), \n",
    "                                                                                  all_metrics[1], \n",
    "                                                                                  all_metrics[2], \n",
    "                                                                                  all_metrics[9]))\n",
    "\n",
    "    actual_metrics_data.append(all_metrics)\n",
    "#     break\n",
    "    \n",
    "metrics_df = pd.DataFrame(actual_metrics_data, columns=actual_metrics_data_columns)\n",
    "metrics_df.to_csv(os.path.join(evaluation_files_loc, 'baseline_models_eval_num_cat_data.csv'), header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08071d41",
   "metadata": {},
   "source": [
    "## Approach 3**\n",
    "\n",
    "- Only for CatBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e4151c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CATBOOST_CLASSIFIER                 :: Train Acc: 0.884125       :: Test Acc: 0.874          :: F1_C1 :: 0.619335\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train_acc': 0.884125,\n",
       " 'test_acc': 0.874,\n",
       " 'roc_auc': 0.736149,\n",
       " 'class_0': {'precision': 0.884241, 'recall': 0.968613, 'f1': 0.924506},\n",
       " 'class_1': {'precision': 0.803922, 'recall': 0.503686, 'f1': 0.619335},\n",
       " 'ks_stats': 0.576256,\n",
       " 'p_value': 0.0,\n",
       " 'tp': 205,\n",
       " 'tn': 1543,\n",
       " 'fp': 50,\n",
       " 'fn': 202}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Read the CSV file into a Pandas DataFrame, using the first column as the index\n",
    "df = pd.read_csv(data_loc, index_col=0)\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Converting type of columns to category\n",
    "df['Geography'] = df['Geography'].astype('category')\n",
    "df['Gender'] = df['Gender'].astype('category')\n",
    "df['Surname'] = df['Surname'].astype('category')\n",
    "\n",
    "# Assigning numerical values and storing it in another columns\n",
    "df['Geography_new'] = df['Geography'].cat.codes\n",
    "df['Gender_new'] = df['Gender'].cat.codes\n",
    "df['Surname_new'] = df['Surname'].cat.codes\n",
    "\n",
    "df['Geography'] = df['Geography_new']\n",
    "df['Gender'] = df['Gender_new']\n",
    "df['Surname'] = df['Surname_new']\n",
    "\n",
    "df.drop(['CustomerId','Surname_new','Geography_new', 'Gender_new'], axis = 1,inplace=True)\n",
    "# df.drop(['CustomerId', 'Surname','Geography', 'Gender'], axis = 1,inplace=True)\n",
    "df.head()\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Split the DataFrame into features and target variables.\n",
    "x,y = features_target_split(df)\n",
    "\n",
    "# Split the features and target variables into training and testing sets.\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y,df)\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Training\n",
    "\n",
    "model_name = model_list[-1]\n",
    "actual_metrics_data = []\n",
    "\n",
    "# Defining these columns as categorical ::\n",
    "# ['Surname','Geography', 'Gender','NumOfProducts', 'HasCrCard', 'IsActiveMember', 'EstimatedSalary']\n",
    "model = CatBoostClassifier(random_state=42, cat_features=[0,2,3,7,8,9])\n",
    "model.fit(x_train, y_train, verbose=False)\n",
    "\n",
    "# Generate predictions\n",
    "y_pred_train, y_pred_test, y_pred_test_proba = prediction(model, x_train, x_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "model_evaluation = Evaluation(y_train, y_test, y_pred_train, y_pred_test, y_pred_test_proba)\n",
    "\n",
    "all_metrics, _ = model_evaluation.main()\n",
    "all_metrics.insert(0, model_name)\n",
    "\n",
    "print(\"{:<35} :: Train Acc: {:<14} :: Test Acc: {:<14} :: F1_C1 :: {}\".format(model_name.upper(), \n",
    "                                                                                  all_metrics[1], \n",
    "                                                                                  all_metrics[2], \n",
    "                                                                                  all_metrics[9]))\n",
    "\n",
    "actual_metrics_data.append(all_metrics)\n",
    "all_metrics_\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278493af",
   "metadata": {},
   "outputs": [],
   "source": [
    "[]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
