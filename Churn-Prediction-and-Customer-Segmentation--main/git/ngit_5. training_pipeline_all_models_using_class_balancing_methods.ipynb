{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d92ecd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "import multiprocessing\n",
    "\n",
    "from imblearn.combine import SMOTEENN, SMOTETomek\n",
    "from imblearn.over_sampling import ADASYN, SMOTE, BorderlineSMOTE, KMeansSMOTE, SMOTEN, SMOTENC, SVMSMOTE\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
    "from imblearn.ensemble import BalancedBaggingClassifier, BalancedRandomForestClassifier, EasyEnsembleClassifier\n",
    "\n",
    "from scipy.stats import ks_2samp\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.metrics import f1_score, roc_auc_score, classification_report, confusion_matrix\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a596abc1",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9a480b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = [ 'logistic_regression','gaussian_naive_bayes',\n",
    "              'support_vector_classifier','ada_boost',\n",
    "              'extra_trees_classifier','gradient_boosting_classifier',\n",
    "              'hist_gradient_boosting_classifier','random_forest_classifier',\n",
    "              'balanced_bagging_classifier','balanced_random_forest_classifier',\n",
    "              'easy_ensemble_classifier','lgbm_classifier','catboost_classifier']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80a9618",
   "metadata": {},
   "source": [
    "### Metrics columns to store evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad70f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_data_columns = ['model','sampling_method','k_neighbour', \n",
    "                        'train_accuracy', 'test_accuracy', 'roc_auc',\n",
    "                        'precision_0', 'recall_0', 'f1_0',\n",
    "                        'precision_1', 'recall_1', 'f1_1', \n",
    "                        'ks_stat', 'p_value', \n",
    "                        'tp', 'tn', \n",
    "                        'fp', 'fn']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64684f0a",
   "metadata": {},
   "source": [
    "### Directory Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa9c6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location to store evaluation csv files\n",
    "evaluation_files_loc = '../files/bos_models_evaluation/'\n",
    "if not os.path.exists(evaluation_files_loc):\n",
    "    os.makedirs(evaluation_files_loc)\n",
    "    \n",
    "# File location of the dataset\n",
    "data_loc = \"../files/Churn_Modelling.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56eae4c",
   "metadata": {},
   "source": [
    "### Best values of neighbour \n",
    "\n",
    "#### [as evaluated in notebook 3 i.e. evaluation_class_balancing_methods_with_baseline_model.ipynb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff428d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_neighbour_values = np.load('../files/class_imbalance_methods_evaluations/best_neighbour_values.npy', \n",
    "                                allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f0fa82",
   "metadata": {},
   "source": [
    "# Data Preperation and Evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e576de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparation and Evaluation\n",
    "\n",
    "def features_target_split(df, target_col='Exited'):\n",
    "    \"\"\"\n",
    "    Split the DataFrame into features and target variables.\n",
    "    \n",
    "    Parameters:\n",
    "        df (DataFrame): The input DataFrame.\n",
    "        target_col (str): The name of the target column. Default is 'Exited'.\n",
    "        \n",
    "    Returns:\n",
    "        x (DataFrame): The features.\n",
    "        y (Series): The target variable.\n",
    "    \"\"\"\n",
    "    # Drop the target column from the DataFrame to get the features\n",
    "    x = df.drop(target_col, axis=1)\n",
    "    \n",
    "    # Assign the target column as the y variable\n",
    "    y = df[target_col]\n",
    "    \n",
    "    # Return the features and target variables\n",
    "    return x,y\n",
    "\n",
    "\n",
    "def train_test_split(x,y,df,target_col='Exited', test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Split the features and target variables into training and testing sets.\n",
    "    \n",
    "    Parameters:\n",
    "        x (DataFrame): The features.\n",
    "        y (Series): The target variable.\n",
    "        df (DataFrame): The original DataFrame.\n",
    "        target_col (str): The name of the target column. Default is 'Exited'.\n",
    "        test_size (float or int): The proportion or absolute number of samples to include in the testing set. Default is 0.2.\n",
    "        random_state (int): The seed used by the random number generator. Default is 42.\n",
    "        \n",
    "    Returns:\n",
    "        x_train (DataFrame): The training set features.\n",
    "        x_test (DataFrame): The testing set features.\n",
    "        y_train (Series): The training set target variable.\n",
    "        y_test (Series): The testing set target variable.\n",
    "    \"\"\"\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    # Split the features and target variables into training and testing sets\n",
    "    # Stratified is being used to maintain the proportion of class [0 and 1] in splits.\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, \n",
    "                                                        test_size=test_size, \n",
    "                                                        random_state=random_state, \n",
    "                                                        stratify=df[target_col])\n",
    "    \n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "\n",
    "def prediction(model, x_train, x_test):\n",
    "    \"\"\"\n",
    "    Generate predictions using a trained logistic regression model.\n",
    "    \n",
    "    Parameters:\n",
    "        log_reg_model (LogisticRegression): The trained logistic regression model.\n",
    "        x_train (array-like or sparse matrix): The training set features.\n",
    "        x_test (array-like or sparse matrix): The testing set features.\n",
    "        \n",
    "    Returns:\n",
    "        y_pred_train (array-like): Predicted labels for the training set.\n",
    "        y_pred_test (array-like): Predicted labels for the testing set.\n",
    "        y_pred_test_proba (array-like): Predicted probabilities for the testing set.\n",
    "    \"\"\"\n",
    "    # Generate predictions for the training set\n",
    "    y_pred_train = model.predict(x_train)\n",
    "    \n",
    "    # Generate predictions for the testing set\n",
    "    y_pred_test = model.predict(x_test)\n",
    "    \n",
    "    # Generate predicted probabilities for the testing set\n",
    "    y_pred_test_proba = model.predict_proba(x_test)\n",
    "    \n",
    "    return y_pred_train, y_pred_test, y_pred_test_proba\n",
    "\n",
    "\n",
    "class Evaluation():\n",
    "    def __init__(self,y_train, y_test, y_pred_train, y_pred_test, y_pred_test_proba):\n",
    "        self.y_train = y_train\n",
    "        self.y_test = y_test\n",
    "        self.y_pred_train = y_pred_train\n",
    "        self.y_pred_test = y_pred_test\n",
    "        self.y_pred_test_proba = y_pred_test_proba\n",
    "    \n",
    "    def __ks_stats_value__(self):\n",
    "        \"\"\"\n",
    "        Calculate the Kolmogorov-Smirnov (KS) statistic and p-value.\n",
    "        \n",
    "        Returns:\n",
    "            ks_stat (float): The KS statistic.\n",
    "            p_value (float): The p-value.\n",
    "        \"\"\"\n",
    "        \n",
    "        # proba_non_churn contains the predicted probabilities for instances that did not churn\n",
    "        proba_non_churn = self.y_pred_test_proba[:,1][self.y_test==0]\n",
    "        \n",
    "        # proba_churn contains the predicted probabilities for instances that actually churned\n",
    "        proba_churn = self.y_pred_test_proba[:,1][self.y_test==1]\n",
    "        \n",
    "        # Calculating Kolmogorov-Smirnov (KS) statistic and p-value\n",
    "        ks_stat, p_value = ks_2samp(proba_non_churn, proba_churn)\n",
    "        return ks_stat, p_value\n",
    "    \n",
    "    def __accuracy_value__(self):\n",
    "        train_accuracy = accuracy_score(self.y_train, self.y_pred_train)\n",
    "        test_accuracy = accuracy_score(self.y_test, self.y_pred_test)\n",
    "        return train_accuracy, test_accuracy\n",
    "\n",
    "    def __prec_rec_f1_value__(self, pos_label):\n",
    "        \"\"\"\n",
    "        Calculate precision, recall, and F1-score for a given label.\n",
    "        \n",
    "        Parameters:\n",
    "            pos_label: The label for which metrics are calculated.\n",
    "        \n",
    "        Returns:\n",
    "            precision (float): Precision score.\n",
    "            recall (float): Recall score.\n",
    "            f1 (float): F1-score.\n",
    "        \"\"\"\n",
    "        precision = precision_score(self.y_test, self.y_pred_test,pos_label=pos_label)\n",
    "        recall = recall_score(self.y_test, self.y_pred_test,pos_label=pos_label)\n",
    "        f1 = f1_score(self.y_test, self.y_pred_test, pos_label=pos_label)\n",
    "        return precision, recall, f1\n",
    "\n",
    "    def __roc_value__(self):\n",
    "        roc_auc = roc_auc_score(self.y_test, self.y_pred_test)\n",
    "        return roc_auc\n",
    "\n",
    "    def __confusion_matrix_value__(self):\n",
    "        tn, fp, fn, tp = confusion_matrix(self.y_test, self.y_pred_test).ravel()\n",
    "        return tn, fp, fn, tp\n",
    "    \n",
    "    def main(self):\n",
    "        train_accuracy, test_accuracy = self.__accuracy_value__()\n",
    "        \n",
    "        precision_0, recall_0, f1_0 = self.__prec_rec_f1_value__(pos_label=0)\n",
    "        precision_1, recall_1, f1_1 = self.__prec_rec_f1_value__(pos_label=1)\n",
    "        \n",
    "        ks_stat, p_value = self.__ks_stats_value__()\n",
    "        \n",
    "        roc_auc = self.__roc_value__()\n",
    "        \n",
    "        tn, fp, fn, tp = self.__confusion_matrix_value__()\n",
    "        \n",
    "        all_metrics = [train_accuracy, test_accuracy, roc_auc, \n",
    "                       precision_0, recall_0, f1_0, \n",
    "                       precision_1, recall_1, f1_1, \n",
    "                       ks_stat, p_value, \n",
    "                       tp, tn, fp, fn]\n",
    "        \n",
    "        all_metrics = [round(value, ndigits=6) for value in all_metrics]\n",
    "        all_metrics_dict = {'train_acc':all_metrics[0], 'test_acc':all_metrics[1], 'roc_auc':all_metrics[2],  \n",
    "                            'class_0':{'precision':all_metrics[3], 'recall':all_metrics[4], 'f1':all_metrics[5]}, \n",
    "                            'class_1':{'precision':all_metrics[6], 'recall':all_metrics[7], 'f1':all_metrics[8]},\n",
    "                            'ks_stats':all_metrics[9], 'p_value':all_metrics[10],\n",
    "                            'tp':all_metrics[11],'tn':all_metrics[12],'fp':all_metrics[13],'fn':all_metrics[14]}\n",
    "        \n",
    "        return all_metrics, all_metrics_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafdbd6a",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2c02bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_model_train(x_train, y_train, random_state=42, max_iter=1000):\n",
    "    \"\"\"\n",
    "    Train a logistic regression model using the provided training data.\n",
    "    \n",
    "    Parameters:\n",
    "        x_train (DataFrame): The training set features.\n",
    "        y_train (Series): The training set target variable.\n",
    "        random_state (int): The seed used by the random number generator. Default is 42.\n",
    "        max_iter (int): The maximum number of iterations for the solver to converge. Default is 1000.\n",
    "        \n",
    "    Returns:\n",
    "        log_reg_model (LogisticRegression): The trained logistic regression model.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create an instance of LogisticRegression model with specified random_state and max_iter\n",
    "    log_reg_model = LogisticRegression(random_state=random_state, max_iter=max_iter)\n",
    "    \n",
    "    # Fit the logistic regression model to the training data\n",
    "    log_reg_model.fit(x_train, y_train)\n",
    "    \n",
    "    return log_reg_model\n",
    "\n",
    "\n",
    "def gnb_model_train(x_train, y_train):\n",
    "    \n",
    "    # instantiate the model\n",
    "    gnb = GaussianNB()\n",
    "    gnb.fit(x_train, y_train)\n",
    "    return gnb\n",
    "\n",
    "def svc_model_train(x_train, y_train, random_state=42):\n",
    "\n",
    "    svc = SVC(probability=True,random_state=random_state)\n",
    "    svc.fit(x_train, y_train)\n",
    "    return svc\n",
    "\n",
    "def adaboost_model_train(x_train, y_train, random_state=42):\n",
    "\n",
    "    adb_model = AdaBoostClassifier(random_state=random_state)\n",
    "    adb_model.fit(x_train, y_train)\n",
    "    return adb_model\n",
    "\n",
    "def etc_model_train(x_train, y_train, random_state=42):\n",
    "    etc_model = ExtraTreesClassifier(random_state=random_state)\n",
    "    etc_model.fit(x_train, y_train)\n",
    "    return etc_model\n",
    "\n",
    "def gbc_model_train(x_train, y_train, random_state=42):\n",
    "    gbc_model = GradientBoostingClassifier(random_state=random_state)\n",
    "    gbc_model.fit(x_train, y_train)\n",
    "    return gbc_model\n",
    "\n",
    "def hgbc_model_train(x_train, y_train, random_state=42):\n",
    "    hgbc_model = HistGradientBoostingClassifier(random_state=random_state)\n",
    "    hgbc_model.fit(x_train, y_train)\n",
    "    return hgbc_model\n",
    "\n",
    "def rfc_model_train(x_train, y_train, random_state=42):\n",
    "    rfc_model = RandomForestClassifier(random_state=random_state)\n",
    "    rfc_model.fit(x_train, y_train)\n",
    "    return rfc_model\n",
    "\n",
    "def bbc_model_train(x_train, y_train, random_state=42):\n",
    "    bbc_model = BalancedBaggingClassifier(random_state=random_state)\n",
    "    bbc_model.fit(x_train, y_train)\n",
    "    return bbc_model\n",
    "\n",
    "def brfc_model_train(x_train, y_train, random_state=42):\n",
    "    brfc_model = BalancedRandomForestClassifier(random_state=random_state)\n",
    "    brfc_model.fit(x_train, y_train)\n",
    "    return brfc_model\n",
    "\n",
    "def eec_model_train(x_train, y_train, random_state=42):\n",
    "    eec_model = EasyEnsembleClassifier(random_state=random_state)\n",
    "    eec_model.fit(x_train, y_train)\n",
    "    return eec_model\n",
    "\n",
    "def lgbm_model_train(x_train, y_train, random_state=42):\n",
    "    lgbm_model = LGBMClassifier(random_state=random_state)\n",
    "    lgbm_model.fit(x_train, y_train)\n",
    "    return lgbm_model\n",
    "\n",
    "def catboost_model_train(x_train, y_train, random_state=42):\n",
    "    catboost_model = CatBoostClassifier(random_state=random_state)\n",
    "    catboost_model.fit(x_train, y_train, verbose=False)\n",
    "    return catboost_model\n",
    "\n",
    "def train_all_models(x_train, y_train, model_name):\n",
    "    \n",
    "    if model_name == 'logistic_regression':\n",
    "        model = logistic_model_train(x_train, y_train)\n",
    "        \n",
    "    elif model_name == 'gaussian_naive_bayes':\n",
    "        model = gnb_model_train(x_train, y_train)\n",
    "        \n",
    "    elif model_name == 'support_vector_classifier':\n",
    "        model = svc_model_train(x_train, y_train)\n",
    "        \n",
    "    elif model_name == 'ada_boost':\n",
    "        model = adaboost_model_train(x_train, y_train)\n",
    "        \n",
    "    elif model_name == 'extra_trees_classifier':\n",
    "        model = etc_model_train(x_train, y_train)\n",
    "\n",
    "    elif model_name == 'gradient_boosting_classifier':\n",
    "        model = gbc_model_train(x_train, y_train)\n",
    "    \n",
    "    elif model_name == 'hist_gradient_boosting_classifier':\n",
    "        model = hgbc_model_train(x_train, y_train)\n",
    "    \n",
    "    elif model_name == 'random_forest_classifier':\n",
    "        model = rfc_model_train(x_train, y_train)\n",
    "\n",
    "    elif model_name == 'balanced_bagging_classifier':\n",
    "        model = bbc_model_train(x_train, y_train)\n",
    "        \n",
    "    elif model_name == 'balanced_random_forest_classifier':\n",
    "        model = brfc_model_train(x_train, y_train)\n",
    "        \n",
    "    elif model_name == 'easy_ensemble_classifier':\n",
    "        model = eec_model_train(x_train, y_train)\n",
    "\n",
    "    elif model_name == 'lgbm_classifier':\n",
    "        model = lgbm_model_train(x_train, y_train)\n",
    "\n",
    "    elif model_name == 'catboost_classifier':\n",
    "        model = catboost_model_train(x_train, y_train)\n",
    "\n",
    "    else:\n",
    "        print(\"Check model name\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1888c901",
   "metadata": {},
   "source": [
    "# Class Balancing Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce132b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smote_method(x,y,neighbour):\n",
    "    # Apply SMOTE\n",
    "    sm = SMOTE(random_state=42, k_neighbors=neighbour)\n",
    "    x_new, y_new = sm.fit_resample(x,y)\n",
    "    \n",
    "    return x_new, y_new\n",
    "\n",
    "def adasyn_method(x,y,neighbour):\n",
    "    adap_synt = ADASYN(random_state=42, n_neighbors=neighbour)\n",
    "    x_new, y_new = adap_synt.fit_resample(x,y)\n",
    "    \n",
    "    return x_new, y_new\n",
    "\n",
    "def borderline_smote_method(x,y,neighbour):\n",
    "    border_smote = BorderlineSMOTE(random_state=42, k_neighbors=neighbour)\n",
    "    x_new, y_new = border_smote.fit_resample(x,y)\n",
    "\n",
    "    return x_new, y_new\n",
    "\n",
    "def kmeans_smote_method(x,y,neighbour):\n",
    "    kmeans_smote = KMeansSMOTE(random_state=42, k_neighbors=neighbour,cluster_balance_threshold=0.2)\n",
    "    x_new, y_new = kmeans_smote.fit_resample(x,y)\n",
    "\n",
    "    return x_new, y_new\n",
    "\n",
    "def smoten_method(x,y,neighbour):\n",
    "    # Apply SMOTEN\n",
    "    sm = SMOTEN(random_state=42, k_neighbors=neighbour)\n",
    "    x_new, y_new = sm.fit_resample(x,y)\n",
    "    \n",
    "    return x_new, y_new\n",
    "\n",
    "def smotenc_method(x,y,neighbour, approach_type):\n",
    "    \n",
    "    # File location of the dataset\n",
    "    data_loc = \"./Churn_Modelling.csv\"\n",
    "\n",
    "    # Read the CSV file into a Pandas DataFrame, using the first column as the index\n",
    "    df = pd.read_csv(data_loc, index_col=0)\n",
    "    \n",
    "    df.drop(['CustomerId'], axis = 1,inplace=True)\n",
    "    \n",
    "    if approach_type == 1:\n",
    "    \n",
    "        x,y = features_target_split(df)\n",
    "\n",
    "        # Apply SMOTENC\n",
    "        sm = SMOTENC(categorical_features=[0,2,3],random_state=42, k_neighbors=neighbour)\n",
    "        x_new, y_new = sm.fit_resample(x,y)\n",
    "\n",
    "        x_new.drop(['Surname', 'Geography', 'Gender'], axis = 1,inplace=True)\n",
    "        \n",
    "        return x_new, y_new\n",
    "    \n",
    "    if approach_type == 2:\n",
    "        x,y = features_target_split(df)\n",
    "        \n",
    "        # Split the features and target variables into training and testing sets.\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x,y,df)\n",
    "\n",
    "        # Apply SMOTENC\n",
    "        sm = SMOTENC(categorical_features=[0,2,3],random_state=42, k_neighbors=neighbour)\n",
    "        x_train_new, y_train_new = sm.fit_resample(x_train,y_train)\n",
    "\n",
    "        x_train_new.drop(['Surname', 'Geography', 'Gender'], axis = 1,inplace=True)\n",
    "        \n",
    "        x_test.drop(['Surname', 'Geography', 'Gender'], axis = 1,inplace=True)\n",
    "        \n",
    "        return x_train_new, x_test, y_train_new, y_test \n",
    "\n",
    "def svm_smote_method(x,y,neighbour):\n",
    "    # Apply SVMSMOTE\n",
    "    sm = SVMSMOTE(random_state=42, k_neighbors=neighbour)\n",
    "    x_new, y_new = sm.fit_resample(x,y)\n",
    "    \n",
    "    return x_new, y_new\n",
    "\n",
    "def smote_enn_method(x,y,neighbour):\n",
    "    # Apply SMOTEENN\n",
    "    sm = SMOTEENN(random_state=42, smote=SMOTE(random_state=42, k_neighbors=neighbour))\n",
    "    x_new, y_new = sm.fit_resample(x,y)\n",
    "    \n",
    "    return x_new, y_new\n",
    "\n",
    "def smote_tomek_method(x,y,neighbour):\n",
    "    # Apply SMOTETOMEK\n",
    "    sm = SMOTETomek(random_state=42, smote=SMOTE(random_state=42, k_neighbors=neighbour))\n",
    "    x_new, y_new = sm.fit_resample(x,y)\n",
    "    \n",
    "    return x_new, y_new\n",
    "\n",
    "def sampling_method(method_name, neighbour, x_train, y_train):\n",
    "    \n",
    "#     print(f\"\\nUsing {method_name.upper()} :: APPROACH 2 :: \")\n",
    "\n",
    "    if method_name == 'smote':\n",
    "        # Apply SMOTE\n",
    "        x_train_new, y_train_new = smote_method(x_train,y_train,neighbour)\n",
    "\n",
    "    if method_name == 'adasyn':\n",
    "        # Apply ADASYN\n",
    "        x_train_new, y_train_new = adasyn_method(x_train,y_train,neighbour)\n",
    "\n",
    "    if method_name == 'borderline_smote':\n",
    "        # Apply Borderline SMOTE\n",
    "        x_train_new, y_train_new = borderline_smote_method(x_train,y_train,neighbour)\n",
    "\n",
    "    if method_name == 'kmeans_smote':\n",
    "        # Apply KMeans SMOTE\n",
    "        x_train_new, y_train_new = kmeans_smote_method(x_train,y_train,neighbour)\n",
    "\n",
    "    if method_name == 'smoten':\n",
    "        # Apply SMOTEN\n",
    "        x_train_new, y_train_new = smoten_method(x_train,y_train,neighbour)\n",
    "\n",
    "    if method_name == 'smotenc':\n",
    "        # Apply SMOTENC\n",
    "        x_train_new, x_test, y_train_new, y_test = smotenc_method(x_train,y_train,neighbour, approach_type=2)\n",
    "\n",
    "    if method_name == 'svmsmote':\n",
    "        # Apply SVMSMOTE\n",
    "        x_train_new, y_train_new = svm_smote_method(x_train,y_train,neighbour)\n",
    "\n",
    "    if method_name == 'smoteenn':\n",
    "        # Apply SMOTEENN\n",
    "        x_train_new, y_train_new = smote_enn_method(x_train,y_train,neighbour)\n",
    "\n",
    "    if method_name == 'smotetomek':\n",
    "        # Apply SMOTETOMEK\n",
    "        x_train_new, y_train_new = smote_tomek_method(x_train,y_train,neighbour)\n",
    "    \n",
    "    \n",
    "    return x_train_new, y_train_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a11d32",
   "metadata": {},
   "source": [
    "# Multiprocessing for class_balancing_methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb9996c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_processing(neighbour):\n",
    "    x_train_new, y_train_new = sampling_method(mth, neighbour, x_train, y_train)\n",
    "\n",
    "    model = train_all_models(x_train_new, y_train_new, model_name)\n",
    "\n",
    "#     # Defining these columns as categorical ::\n",
    "#     # ['Surname','Geography', 'Gender','NumOfProducts', 'HasCrCard', 'IsActiveMember', 'EstimatedSalary']\n",
    "#     model = CatBoostClassifier(random_state=42, cat_features=[0,2,3,7,8,9])\n",
    "#     model.fit(x_train, y_train, verbose=False)\n",
    "\n",
    "    # Generate predictions\n",
    "    y_pred_train, y_pred_test, y_pred_test_proba = prediction(model, x_train_new, x_test)\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    model_evaluation = Evaluation(y_train_new, y_test, y_pred_train, y_pred_test, y_pred_test_proba)\n",
    "\n",
    "    all_metrics,_ = model_evaluation.main()\n",
    "    all_metrics.insert(0, model_name)\n",
    "    all_metrics.insert(1, mth)\n",
    "    all_metrics.insert(2, neighbour)\n",
    "\n",
    "#     if neighbour%100 == 0 or neighbour == 1:\n",
    "#     print(\"{:<6} :: Train Acc: {:<14} :: Test Acc: {}\".format(neighbour, all_metrics[3], all_metrics[4]))\n",
    "#         print('.', end='')   \n",
    "    return all_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb968c66",
   "metadata": {},
   "source": [
    "# Start Point\n",
    "\n",
    "Three approach is used to train all the models:\n",
    "\n",
    "**APPROACH 1**: Using numerical features only.\n",
    "\n",
    "**APPROACH 2**: Using categorical and numerical features. [Label Encoding is used]\n",
    "\n",
    "**APPROACH 3**: This is used only for CatBoost model as it supports to mark the columns as categorical."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4505e91a",
   "metadata": {},
   "source": [
    "## Approach 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f477592",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Read the CSV file into a Pandas DataFrame, using the first column as the index\n",
    "df = pd.read_csv(data_loc, index_col=0)\n",
    "\n",
    "# Drop all categorical columns\n",
    "df.drop(['CustomerId', 'Surname','Geography', 'Gender'], axis = 1,inplace=True)\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Split the DataFrame into features and target variables.\n",
    "x,y = features_target_split(df)\n",
    "\n",
    "# Split the features and target variables into training and testing sets.\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y,df)\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Training\n",
    "\n",
    "for model_name in model_list:\n",
    "    metrics_data = []\n",
    "    print(model_name.upper())\n",
    "    \n",
    "    for mth in methods_:\n",
    "#         print(f\"\\t{mth.upper()}\", end='')\n",
    "        start_time = time()\n",
    "        \n",
    "        # Reading best value of k neighbour from the best_neighbour_values.npy\n",
    "        if mth == 'borderline_smote':\n",
    "            all_neighbour = best_neighbour_values['borderline'.upper()]\n",
    "        elif mth == 'kmeans_smote':\n",
    "            all_neighbour = best_neighbour_values['kmeans'.upper()]\n",
    "        else:\n",
    "            all_neighbour = best_neighbour_values[mth.upper()]\n",
    "        \n",
    "        # Create a multiprocessing pool\n",
    "        pool = multiprocessing.Pool()\n",
    "        # Iterate over model_list and methods_ to process each combination in parallel\n",
    "        all_metrics = pool.map(multi_processing, all_neighbour,)\n",
    "\n",
    "        # Close the pool and wait for all processes to complete\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        \n",
    "        metrics_data = metrics_data + all_metrics\n",
    "        end_time = time()\n",
    "#         print(f\"\\t{mth.upper()} :: k_neighbour :: {len(all_neighbour)} :: Time :: {round(end_time-start_time, ndigits=4)}\")\n",
    "        print(\"\\t{:<20} :: Top_k_neighbour_count :: {:<5} :: Time :: {}\".format(mth.upper(),len(all_neighbour), round(end_time-start_time, ndigits=4)))\n",
    "    print()   \n",
    "    metrics_df = pd.DataFrame(metrics_data, columns=metrics_data_columns)\n",
    "    metrics_df.to_csv(os.path.join(evaluation_files_loc, f'bos_{model_name}_eval_num_data.csv'), header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3095507b",
   "metadata": {},
   "source": [
    "## Approach 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a765cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------------------------------\n",
    "# Read the CSV file into a Pandas DataFrame, using the first column as the index\n",
    "df = pd.read_csv(data_loc, index_col=0)\n",
    "\n",
    "# Converting type of columns to category\n",
    "df['Geography'] = df['Geography'].astype('category')\n",
    "df['Gender'] = df['Gender'].astype('category')\n",
    "df['Surname'] = df['Surname'].astype('category')\n",
    "\n",
    "# Assigning numerical values and storing it in another columns\n",
    "df['Geography_new'] = df['Geography'].cat.codes\n",
    "df['Gender_new'] = df['Gender'].cat.codes\n",
    "df['Surname_new'] = df['Surname'].cat.codes\n",
    "\n",
    "df['Geography'] = df['Geography_new']\n",
    "df['Gender'] = df['Gender_new']\n",
    "df['Surname'] = df['Surname_new']\n",
    "\n",
    "df.drop(['CustomerId', 'Surname_new','Geography_new', 'Gender_new'], axis = 1,inplace=True)\n",
    "# df.drop(['CustomerId', 'Surname','Geography', 'Gender'], axis = 1,inplace=True)\n",
    "df.head()\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Split the DataFrame into features and target variables.\n",
    "x,y = features_target_split(df)\n",
    "\n",
    "# Split the features and target variables into training and testing sets.\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y,df)\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Training\n",
    "\n",
    "for model_name in model_list:\n",
    "    metrics_data = []\n",
    "    print(model_name.upper())\n",
    "    \n",
    "    for mth in methods_:\n",
    "#         print(f\"\\t{mth.upper()}\", end='')\n",
    "        start_time = time()\n",
    "\n",
    "        # Reading best value of k neighbour from the best_neighbour_values.npy\n",
    "        if mth == 'borderline_smote':\n",
    "            all_neighbour = best_neighbour_values['borderline'.upper()]\n",
    "        elif mth == 'kmeans_smote':\n",
    "            all_neighbour = best_neighbour_values['kmeans'.upper()]\n",
    "        else:\n",
    "            all_neighbour = best_neighbour_values[mth.upper()]\n",
    "        \n",
    "        # Create a multiprocessing pool\n",
    "        pool = multiprocessing.Pool()\n",
    "        # Iterate over model_list and methods_ to process each combination in parallel\n",
    "        all_metrics = pool.map(multi_processing, all_neighbour)\n",
    "\n",
    "        # Close the pool and wait for all processes to complete\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        \n",
    "        metrics_data = metrics_data + all_metrics\n",
    "        end_time = time()\n",
    "#         print(f\" :: Time :: {round(end_time-start_time, ndigits=4)}\")\n",
    "        print(\"\\t{:<20} :: Top_k_neighbour_count :: {:<5} :: Time :: {}\".format(mth.upper(),len(all_neighbour), round(end_time-start_time, ndigits=4)))\n",
    "    print()   \n",
    "    metrics_df = pd.DataFrame(metrics_data, columns=metrics_data_columns)\n",
    "    metrics_df.to_csv(os.path.join(evaluation_files_loc, f'bos_{model_name}_eval_num_cat_data.csv'), header=True, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d614e2",
   "metadata": {},
   "source": [
    "## Approach 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2083c577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------------------------------\n",
    "# Read the CSV file into a Pandas DataFrame, using the first column as the index\n",
    "df = pd.read_csv(data_loc, index_col=0)\n",
    "\n",
    "# Converting type of columns to category\n",
    "df['Geography'] = df['Geography'].astype('category')\n",
    "df['Gender'] = df['Gender'].astype('category')\n",
    "df['Surname'] = df['Surname'].astype('category')\n",
    "\n",
    "# Assigning numerical values and storing it in another columns\n",
    "df['Geography_new'] = df['Geography'].cat.codes\n",
    "df['Gender_new'] = df['Gender'].cat.codes\n",
    "df['Surname_new'] = df['Surname'].cat.codes\n",
    "\n",
    "df['Geography'] = df['Geography_new']\n",
    "df['Gender'] = df['Gender_new']\n",
    "df['Surname'] = df['Surname_new']\n",
    "\n",
    "df.drop(['CustomerId', 'Surname_new','Geography_new', 'Gender_new'], axis = 1,inplace=True)\n",
    "# df.drop(['CustomerId', 'Surname','Geography', 'Gender'], axis = 1,inplace=True)\n",
    "df.head()\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Split the DataFrame into features and target variables.\n",
    "x,y = features_target_split(df)\n",
    "\n",
    "# Split the features and target variables into training and testing sets.\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y,df)\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Training\n",
    "\n",
    "model_name = model_list[-1]\n",
    "\n",
    "metrics_data = []\n",
    "print(model_name.upper())\n",
    "\n",
    "for mth in methods_:\n",
    "\n",
    "    start_time = time()\n",
    "\n",
    "    # Reading best value of k neighbour from the best_neighbour_values.npy\n",
    "    if mth == 'borderline_smote':\n",
    "        all_neighbour = best_neighbour_values['borderline'.upper()]\n",
    "    elif mth == 'kmeans_smote':\n",
    "        all_neighbour = best_neighbour_values['kmeans'.upper()]\n",
    "    else:\n",
    "        all_neighbour = best_neighbour_values[mth.upper()]\n",
    "\n",
    "    # Create a multiprocessing pool\n",
    "    pool = multiprocessing.Pool()\n",
    "    # Iterate over model_list and methods_ to process each combination in parallel\n",
    "    all_metrics = pool.map(multi_processing, all_neighbour)\n",
    "\n",
    "    # Close the pool and wait for all processes to complete\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    metrics_data = metrics_data + all_metrics\n",
    "    end_time = time()\n",
    "#         print(f\" :: Time :: {round(end_time-start_time, ndigits=4)}\")\n",
    "    print(\"\\t{:<20} :: Top_k_neighbour_count :: {:<5} :: Time :: {}\".format(mth.upper(),len(all_neighbour), round(end_time-start_time, ndigits=4)))\n",
    "print()   \n",
    "metrics_df = pd.DataFrame(metrics_data, columns=metrics_data_columns)\n",
    "metrics_df.to_csv(os.path.join(evaluation_files_loc, f'bos_{model_name}_eval_num_cat_as_cat_data.csv'), header=True, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
