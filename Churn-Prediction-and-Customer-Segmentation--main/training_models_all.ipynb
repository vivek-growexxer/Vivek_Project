{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f3b5165",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from time import time\n",
    "\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "from imblearn.over_sampling import ADASYN, SMOTE, BorderlineSMOTE, KMeansSMOTE, SMOTEN, SMOTENC, SVMSMOTE\n",
    "from imblearn.combine import SMOTEENN, SMOTETomek\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "from imblearn.ensemble import BalancedBaggingClassifier, BalancedRandomForestClassifier, EasyEnsembleClassifier\n",
    "\n",
    "\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.metrics import f1_score, roc_auc_score, classification_report, confusion_matrix\n",
    "\n",
    "metrics_data_columns = ['model','sampling_method','k_neighbour', 'train_accuracy', 'test_accuracy', 'roc_auc',\n",
    "                         'precision_0', 'recall_0', 'f1_0',\n",
    "                         'precision_1', 'recall_1', 'f1_1', \n",
    "                         'ks_stat', 'p_value', \n",
    "                         'tp', 'tn', 'fp', 'fn']\n",
    "\n",
    "\n",
    "# model_list = [ 'logistic_regression','gaussian_naive_bayes','support_vector_classifier',\n",
    "#               'ada_boost','extra_trees_classifier','gradient_boosting_classifier',\n",
    "#               'hist_gradient_boosting_classifier','random_forest_classifier',\n",
    "#               'balanced_bagging_classifier','balanced_random_forest_classifier',\n",
    "#               'easy_ensemble_classifier','lgbm_classifier','catboost_classifier']\n",
    "\n",
    "model_list = [ 'gaussian_naive_bayes', 'ada_boost','extra_trees_classifier',\n",
    "              'gradient_boosting_classifier', 'hist_gradient_boosting_classifier',\n",
    "              'random_forest_classifier','balanced_bagging_classifier',\n",
    "              'balanced_random_forest_classifier','easy_ensemble_classifier',\n",
    "              'lgbm_classifier','catboost_classifier']\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb69e4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_files_loc = './final_results/bos_models_evaluation/'\n",
    "if not os.path.exists(evaluation_files_loc):\n",
    "    os.makedirs(evaluation_files_loc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694c1e10",
   "metadata": {},
   "source": [
    "## Data preperation and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63f6c32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparation and Evaluation\n",
    "\n",
    "def features_target_split(df, target_col='Exited'):\n",
    "    \"\"\"\n",
    "    Split the DataFrame into features and target variables.\n",
    "    \n",
    "    Parameters:\n",
    "        df (DataFrame): The input DataFrame.\n",
    "        target_col (str): The name of the target column. Default is 'Exited'.\n",
    "        \n",
    "    Returns:\n",
    "        x (DataFrame): The features.\n",
    "        y (Series): The target variable.\n",
    "    \"\"\"\n",
    "    # Drop the target column from the DataFrame to get the features\n",
    "    x = df.drop(target_col, axis=1)\n",
    "    \n",
    "    # Assign the target column as the y variable\n",
    "    y = df[target_col]\n",
    "    \n",
    "    # Return the features and target variables\n",
    "    return x,y\n",
    "\n",
    "\n",
    "def train_test_split(x,y,df,target_col='Exited', test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Split the features and target variables into training and testing sets.\n",
    "    \n",
    "    Parameters:\n",
    "        x (DataFrame): The features.\n",
    "        y (Series): The target variable.\n",
    "        df (DataFrame): The original DataFrame.\n",
    "        target_col (str): The name of the target column. Default is 'Exited'.\n",
    "        test_size (float or int): The proportion or absolute number of samples to include in the testing set. Default is 0.2.\n",
    "        random_state (int): The seed used by the random number generator. Default is 42.\n",
    "        \n",
    "    Returns:\n",
    "        x_train (DataFrame): The training set features.\n",
    "        x_test (DataFrame): The testing set features.\n",
    "        y_train (Series): The training set target variable.\n",
    "        y_test (Series): The testing set target variable.\n",
    "    \"\"\"\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    # Split the features and target variables into training and testing sets\n",
    "    # Stratified is being used to maintain the proportion of class [0 and 1] in splits.\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, \n",
    "                                                        test_size=test_size, \n",
    "                                                        random_state=random_state, \n",
    "                                                        stratify=df[target_col])\n",
    "    \n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "\n",
    "def prediction(model, x_train, x_test):\n",
    "    \"\"\"\n",
    "    Generate predictions using a trained logistic regression model.\n",
    "    \n",
    "    Parameters:\n",
    "        log_reg_model (LogisticRegression): The trained logistic regression model.\n",
    "        x_train (array-like or sparse matrix): The training set features.\n",
    "        x_test (array-like or sparse matrix): The testing set features.\n",
    "        \n",
    "    Returns:\n",
    "        y_pred_train (array-like): Predicted labels for the training set.\n",
    "        y_pred_test (array-like): Predicted labels for the testing set.\n",
    "        y_pred_test_proba (array-like): Predicted probabilities for the testing set.\n",
    "    \"\"\"\n",
    "    # Generate predictions for the training set\n",
    "    y_pred_train = model.predict(x_train)\n",
    "    \n",
    "    # Generate predictions for the testing set\n",
    "    y_pred_test = model.predict(x_test)\n",
    "    \n",
    "    # Generate predicted probabilities for the testing set\n",
    "    y_pred_test_proba = model.predict_proba(x_test)\n",
    "    \n",
    "    return y_pred_train, y_pred_test, y_pred_test_proba\n",
    "\n",
    "\n",
    "class Evaluation():\n",
    "    def __init__(self,y_train, y_test, y_pred_train, y_pred_test, y_pred_test_proba):\n",
    "        self.y_train = y_train\n",
    "        self.y_test = y_test\n",
    "        self.y_pred_train = y_pred_train\n",
    "        self.y_pred_test = y_pred_test\n",
    "        self.y_pred_test_proba = y_pred_test_proba\n",
    "    \n",
    "    def __ks_stats_value__(self):\n",
    "        \"\"\"\n",
    "        Calculate the Kolmogorov-Smirnov (KS) statistic and p-value.\n",
    "        \n",
    "        Returns:\n",
    "            ks_stat (float): The KS statistic.\n",
    "            p_value (float): The p-value.\n",
    "        \"\"\"\n",
    "        \n",
    "        # proba_non_churn contains the predicted probabilities for instances that did not churn\n",
    "        proba_non_churn = self.y_pred_test_proba[:,1][self.y_test==0]\n",
    "        \n",
    "        # proba_churn contains the predicted probabilities for instances that actually churned\n",
    "        proba_churn = self.y_pred_test_proba[:,1][self.y_test==1]\n",
    "        \n",
    "        # Calculating Kolmogorov-Smirnov (KS) statistic and p-value\n",
    "        ks_stat, p_value = ks_2samp(proba_non_churn, proba_churn)\n",
    "        return ks_stat, p_value\n",
    "    \n",
    "    def __accuracy_value__(self):\n",
    "        train_accuracy = accuracy_score(self.y_train, self.y_pred_train)\n",
    "        test_accuracy = accuracy_score(self.y_test, self.y_pred_test)\n",
    "        return train_accuracy, test_accuracy\n",
    "\n",
    "    def __prec_rec_f1_value__(self, pos_label):\n",
    "        \"\"\"\n",
    "        Calculate precision, recall, and F1-score for a given label.\n",
    "        \n",
    "        Parameters:\n",
    "            pos_label: The label for which metrics are calculated.\n",
    "        \n",
    "        Returns:\n",
    "            precision (float): Precision score.\n",
    "            recall (float): Recall score.\n",
    "            f1 (float): F1-score.\n",
    "        \"\"\"\n",
    "        precision = precision_score(self.y_test, self.y_pred_test,pos_label=pos_label)\n",
    "        recall = recall_score(self.y_test, self.y_pred_test,pos_label=pos_label)\n",
    "        f1 = f1_score(self.y_test, self.y_pred_test, pos_label=pos_label)\n",
    "        return precision, recall, f1\n",
    "\n",
    "    def __roc_value__(self):\n",
    "        roc_auc = roc_auc_score(self.y_test, self.y_pred_test)\n",
    "        return roc_auc\n",
    "\n",
    "    def __confusion_matrix_value__(self):\n",
    "        tn, fp, fn, tp = confusion_matrix(self.y_test, self.y_pred_test).ravel()\n",
    "        return tn, fp, fn, tp\n",
    "    \n",
    "    def main(self):\n",
    "        train_accuracy, test_accuracy = self.__accuracy_value__()\n",
    "        \n",
    "        precision_0, recall_0, f1_0 = self.__prec_rec_f1_value__(pos_label=0)\n",
    "        precision_1, recall_1, f1_1 = self.__prec_rec_f1_value__(pos_label=1)\n",
    "        \n",
    "        ks_stat, p_value = self.__ks_stats_value__()\n",
    "        \n",
    "        roc_auc = self.__roc_value__()\n",
    "        \n",
    "        tn, fp, fn, tp = self.__confusion_matrix_value__()\n",
    "        \n",
    "        all_metrics = [train_accuracy, test_accuracy, roc_auc, \n",
    "                       precision_0, recall_0, f1_0, \n",
    "                       precision_1, recall_1, f1_1, \n",
    "                       ks_stat, p_value, \n",
    "                       tp, tn, fp, fn]\n",
    "        \n",
    "        all_metrics = [round(value, ndigits=6) for value in all_metrics]\n",
    "        all_metrics_dict = {'train_acc':all_metrics[0], 'test_acc':all_metrics[1], 'roc_auc':all_metrics[2],  \n",
    "                            'class_0':{'precision':all_metrics[3], 'recall':all_metrics[4], 'f1':all_metrics[5]}, \n",
    "                            'class_1':{'precision':all_metrics[6], 'recall':all_metrics[7], 'f1':all_metrics[8]},\n",
    "                            'ks_stats':all_metrics[9], 'p_value':all_metrics[10],\n",
    "                            'tp':all_metrics[11],'tn':all_metrics[12],'fp':all_metrics[13],'fn':all_metrics[14]}\n",
    "        \n",
    "        return all_metrics, all_metrics_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d715555",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b11bd46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_model_train(x_train, y_train, random_state=42, max_iter=1000):\n",
    "    \"\"\"\n",
    "    Train a logistic regression model using the provided training data.\n",
    "    \n",
    "    Parameters:\n",
    "        x_train (DataFrame): The training set features.\n",
    "        y_train (Series): The training set target variable.\n",
    "        random_state (int): The seed used by the random number generator. Default is 42.\n",
    "        max_iter (int): The maximum number of iterations for the solver to converge. Default is 1000.\n",
    "        \n",
    "    Returns:\n",
    "        log_reg_model (LogisticRegression): The trained logistic regression model.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create an instance of LogisticRegression model with specified random_state and max_iter\n",
    "    log_reg_model = LogisticRegression(random_state=random_state, max_iter=max_iter)\n",
    "    \n",
    "    # Fit the logistic regression model to the training data\n",
    "    log_reg_model.fit(x_train, y_train)\n",
    "    \n",
    "    return log_reg_model\n",
    "\n",
    "\n",
    "def gnb_model_train(x_train, y_train):\n",
    "    \n",
    "    # instantiate the model\n",
    "    gnb = GaussianNB()\n",
    "    gnb.fit(x_train, y_train)\n",
    "    return gnb\n",
    "\n",
    "def svc_model_train(x_train, y_train, random_state=42):\n",
    "\n",
    "    svc = SVC(probability=True,random_state=random_state)\n",
    "    svc.fit(x_train, y_train)\n",
    "    return svc\n",
    "\n",
    "def adaboost_model_train(x_train, y_train, random_state=42):\n",
    "\n",
    "    adb_model = AdaBoostClassifier(random_state=random_state)\n",
    "    adb_model.fit(x_train, y_train)\n",
    "    return adb_model\n",
    "\n",
    "def etc_model_train(x_train, y_train, random_state=42):\n",
    "    etc_model = ExtraTreesClassifier(random_state=random_state)\n",
    "    etc_model.fit(x_train, y_train)\n",
    "    return etc_model\n",
    "\n",
    "def gbc_model_train(x_train, y_train, random_state=42):\n",
    "    gbc_model = GradientBoostingClassifier(random_state=random_state)\n",
    "    gbc_model.fit(x_train, y_train)\n",
    "    return gbc_model\n",
    "\n",
    "def hgbc_model_train(x_train, y_train, random_state=42):\n",
    "    hgbc_model = HistGradientBoostingClassifier(random_state=random_state)\n",
    "    hgbc_model.fit(x_train, y_train)\n",
    "    return hgbc_model\n",
    "\n",
    "def rfc_model_train(x_train, y_train, random_state=42):\n",
    "    rfc_model = RandomForestClassifier(random_state=random_state)\n",
    "    rfc_model.fit(x_train, y_train)\n",
    "    return rfc_model\n",
    "\n",
    "def bbc_model_train(x_train, y_train, random_state=42):\n",
    "    bbc_model = BalancedBaggingClassifier(random_state=random_state)\n",
    "    bbc_model.fit(x_train, y_train)\n",
    "    return bbc_model\n",
    "\n",
    "def brfc_model_train(x_train, y_train, random_state=42):\n",
    "    brfc_model = BalancedRandomForestClassifier(random_state=random_state)\n",
    "    brfc_model.fit(x_train, y_train)\n",
    "    return brfc_model\n",
    "\n",
    "def eec_model_train(x_train, y_train, random_state=42):\n",
    "    eec_model = EasyEnsembleClassifier(random_state=random_state)\n",
    "    eec_model.fit(x_train, y_train)\n",
    "    return eec_model\n",
    "\n",
    "def lgbm_model_train(x_train, y_train, random_state=42):\n",
    "    lgbm_model = LGBMClassifier(random_state=random_state)\n",
    "    lgbm_model.fit(x_train, y_train)\n",
    "    return lgbm_model\n",
    "\n",
    "def catboost_model_train(x_train, y_train, random_state=42):\n",
    "    catboost_model = CatBoostClassifier(random_state=random_state)\n",
    "    catboost_model.fit(x_train, y_train, verbose=False)\n",
    "    return catboost_model\n",
    "\n",
    "def train_all_models(x_train, y_train, model_name):\n",
    "    \n",
    "    if model_name == 'logistic_regression':\n",
    "        model = logistic_model_train(x_train, y_train)\n",
    "        \n",
    "    elif model_name == 'gaussian_naive_bayes':\n",
    "        model = gnb_model_train(x_train, y_train)\n",
    "        \n",
    "    elif model_name == 'support_vector_classifier':\n",
    "        model = svc_model_train(x_train, y_train)\n",
    "        \n",
    "    elif model_name == 'ada_boost':\n",
    "        model = adaboost_model_train(x_train, y_train)\n",
    "        \n",
    "    elif model_name == 'extra_trees_classifier':\n",
    "        model = etc_model_train(x_train, y_train)\n",
    "\n",
    "    elif model_name == 'gradient_boosting_classifier':\n",
    "        model = gbc_model_train(x_train, y_train)\n",
    "    \n",
    "    elif model_name == 'hist_gradient_boosting_classifier':\n",
    "        model = hgbc_model_train(x_train, y_train)\n",
    "    \n",
    "    elif model_name == 'random_forest_classifier':\n",
    "        model = rfc_model_train(x_train, y_train)\n",
    "\n",
    "    elif model_name == 'balanced_bagging_classifier':\n",
    "        model = bbc_model_train(x_train, y_train)\n",
    "        \n",
    "    elif model_name == 'balanced_random_forest_classifier':\n",
    "        model = brfc_model_train(x_train, y_train)\n",
    "        \n",
    "    elif model_name == 'easy_ensemble_classifier':\n",
    "        model = eec_model_train(x_train, y_train)\n",
    "\n",
    "    elif model_name == 'lgbm_classifier':\n",
    "        model = lgbm_model_train(x_train, y_train)\n",
    "\n",
    "    elif model_name == 'catboost_classifier':\n",
    "        model = catboost_model_train(x_train, y_train)\n",
    "\n",
    "    else:\n",
    "        print(\"Check model name\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfede73",
   "metadata": {},
   "source": [
    "## Class Balancing Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4cdb8d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smote_method(x,y,neighbour):\n",
    "    # Apply SMOTE\n",
    "    sm = SMOTE(random_state=42, k_neighbors=neighbour)\n",
    "    x_new, y_new = sm.fit_resample(x,y)\n",
    "    \n",
    "    return x_new, y_new\n",
    "\n",
    "def adasyn_method(x,y,neighbour):\n",
    "    adap_synt = ADASYN(random_state=42, n_neighbors=neighbour)\n",
    "    x_new, y_new = adap_synt.fit_resample(x,y)\n",
    "    \n",
    "    return x_new, y_new\n",
    "\n",
    "def borderline_smote_method(x,y,neighbour):\n",
    "    border_smote = BorderlineSMOTE(random_state=42, k_neighbors=neighbour)\n",
    "    x_new, y_new = border_smote.fit_resample(x,y)\n",
    "\n",
    "    return x_new, y_new\n",
    "\n",
    "def kmeans_smote_method(x,y,neighbour):\n",
    "    kmeans_smote = KMeansSMOTE(random_state=42, k_neighbors=neighbour,cluster_balance_threshold=0.2)\n",
    "    x_new, y_new = kmeans_smote.fit_resample(x,y)\n",
    "\n",
    "    return x_new, y_new\n",
    "\n",
    "def smoten_method(x,y,neighbour):\n",
    "    # Apply SMOTEN\n",
    "    sm = SMOTEN(random_state=42, k_neighbors=neighbour)\n",
    "    x_new, y_new = sm.fit_resample(x,y)\n",
    "    \n",
    "    return x_new, y_new\n",
    "\n",
    "def smotenc_method(x,y,neighbour, approach_type):\n",
    "    \n",
    "    # File location of the dataset\n",
    "    data_loc = \"./Churn_Modelling.csv\"\n",
    "\n",
    "    # Read the CSV file into a Pandas DataFrame, using the first column as the index\n",
    "    df = pd.read_csv(data_loc, index_col=0)\n",
    "    \n",
    "    df.drop(['CustomerId'], axis = 1,inplace=True)\n",
    "    \n",
    "    if approach_type == 1:\n",
    "    \n",
    "        x,y = features_target_split(df)\n",
    "\n",
    "        # Apply SMOTENC\n",
    "        sm = SMOTENC(categorical_features=[0,2,3],random_state=42, k_neighbors=neighbour)\n",
    "        x_new, y_new = sm.fit_resample(x,y)\n",
    "\n",
    "        x_new.drop(['Surname', 'Geography', 'Gender'], axis = 1,inplace=True)\n",
    "        \n",
    "        return x_new, y_new\n",
    "    \n",
    "    if approach_type == 2:\n",
    "        x,y = features_target_split(df)\n",
    "        \n",
    "        # Split the features and target variables into training and testing sets.\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x,y,df)\n",
    "\n",
    "        # Apply SMOTENC\n",
    "        sm = SMOTENC(categorical_features=[0,2,3],random_state=42, k_neighbors=neighbour)\n",
    "        x_train_new, y_train_new = sm.fit_resample(x_train,y_train)\n",
    "\n",
    "        x_train_new.drop(['Surname', 'Geography', 'Gender'], axis = 1,inplace=True)\n",
    "        \n",
    "        x_test.drop(['Surname', 'Geography', 'Gender'], axis = 1,inplace=True)\n",
    "        \n",
    "        return x_train_new, x_test, y_train_new, y_test \n",
    "\n",
    "def svm_smote_method(x,y,neighbour):\n",
    "    # Apply SVMSMOTE\n",
    "    sm = SVMSMOTE(random_state=42, k_neighbors=neighbour)\n",
    "    x_new, y_new = sm.fit_resample(x,y)\n",
    "    \n",
    "    return x_new, y_new\n",
    "\n",
    "def smote_enn_method(x,y,neighbour):\n",
    "    # Apply SMOTEENN\n",
    "    sm = SMOTEENN(random_state=42, smote=SMOTE(random_state=42, k_neighbors=neighbour))\n",
    "    x_new, y_new = sm.fit_resample(x,y)\n",
    "    \n",
    "    return x_new, y_new\n",
    "\n",
    "def smote_tomek_method(x,y,neighbour):\n",
    "    # Apply SMOTETOMEK\n",
    "    sm = SMOTETomek(random_state=42, smote=SMOTE(random_state=42, k_neighbors=neighbour))\n",
    "    x_new, y_new = sm.fit_resample(x,y)\n",
    "    \n",
    "    return x_new, y_new\n",
    "\n",
    "def sampling_method(method_name, neighbour, x_train, y_train):\n",
    "    \n",
    "#     print(f\"\\nUsing {method_name.upper()} :: APPROACH 2 :: \")\n",
    "\n",
    "    if method_name == 'smote':\n",
    "        # Apply SMOTE\n",
    "        x_train_new, y_train_new = smote_method(x_train,y_train,neighbour)\n",
    "\n",
    "    if method_name == 'adasyn':\n",
    "        # Apply ADASYN\n",
    "        x_train_new, y_train_new = adasyn_method(x_train,y_train,neighbour)\n",
    "\n",
    "    if method_name == 'borderline_smote':\n",
    "        # Apply Borderline SMOTE\n",
    "        x_train_new, y_train_new = borderline_smote_method(x_train,y_train,neighbour)\n",
    "\n",
    "    if method_name == 'kmeans_smote':\n",
    "        # Apply KMeans SMOTE\n",
    "        x_train_new, y_train_new = kmeans_smote_method(x_train,y_train,neighbour)\n",
    "\n",
    "    if method_name == 'smoten':\n",
    "        # Apply SMOTEN\n",
    "        x_train_new, y_train_new = smoten_method(x_train,y_train,neighbour)\n",
    "\n",
    "#     if method_name == 'smotenc':\n",
    "#         # Apply SMOTENC\n",
    "#         x_train_new, x_test, y_train_new, y_test = smotenc_method(x_train,y_train,neighbour, approach_type=2)\n",
    "\n",
    "    if method_name == 'svmsmote':\n",
    "        # Apply SVMSMOTE\n",
    "        x_train_new, y_train_new = svm_smote_method(x_train,y_train,neighbour)\n",
    "\n",
    "    if method_name == 'smoteenn':\n",
    "        # Apply SMOTEENN\n",
    "        x_train_new, y_train_new = smote_enn_method(x_train,y_train,neighbour)\n",
    "\n",
    "    if method_name == 'smotetomek':\n",
    "        # Apply SMOTETOMEK\n",
    "        x_train_new, y_train_new = smote_tomek_method(x_train,y_train,neighbour)\n",
    "    \n",
    "    \n",
    "    return x_train_new, y_train_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b6a5b6",
   "metadata": {},
   "source": [
    "## Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5664807c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Exited</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RowNumber</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>619</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>608</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>502</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>699</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>850</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           CreditScore  Age  Tenure    Balance  NumOfProducts  HasCrCard  \\\n",
       "RowNumber                                                                  \n",
       "1                  619   42       2       0.00              1          1   \n",
       "2                  608   41       1   83807.86              1          0   \n",
       "3                  502   42       8  159660.80              3          1   \n",
       "4                  699   39       1       0.00              2          0   \n",
       "5                  850   43       2  125510.82              1          1   \n",
       "\n",
       "           IsActiveMember  EstimatedSalary  Exited  \n",
       "RowNumber                                           \n",
       "1                       1        101348.88       1  \n",
       "2                       1        112542.58       0  \n",
       "3                       0        113931.57       1  \n",
       "4                       0         93826.63       0  \n",
       "5                       1         79084.10       0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# File location of the dataset\n",
    "data_loc = \"./Churn_Modelling.csv\"\n",
    "\n",
    "# Read the CSV file into a Pandas DataFrame, using the first column as the index\n",
    "df = pd.read_csv(data_loc, index_col=0)\n",
    "\n",
    "# df.head()\n",
    "\n",
    "# Drop all categorical columns\n",
    "df.drop(['CustomerId', 'Surname','Geography', 'Gender'], axis = 1,inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be656b1",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3806d9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the DataFrame into features and target variables.\n",
    "x,y = features_target_split(df)\n",
    "\n",
    "# Split the features and target variables into training and testing sets.\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y,df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df4dbfb",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bbe733f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "methods_ = ['smote', 'adasyn', 'borderline_smote', 'kmeans_smote', 'smoten' , 'svmsmote', 'smoteenn', 'smotetomek']\n",
    "# k_values = [273, 1020, 1409, 110, 282, 148, 1, 116]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2023e0db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ac0088d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAUSSIAN_NAIVE_BAYES\n",
      "1      :: Train Acc: 0.716641       :: Test Acc: 0.683\n",
      "500    :: Train Acc: 0.712009       :: Test Acc: 0.6795\n",
      "1000   :: Train Acc: 0.713972       :: Test Acc: 0.6805\n",
      "1500   :: Train Acc: 0.719937       :: Test Acc: 0.68\n",
      "1      :: Train Acc: 0.714469       :: Test Acc: 0.6605\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 19\u001b[0m\n\u001b[1;32m     15\u001b[0m     stop_range \u001b[38;5;241m=\u001b[39m y_train\u001b[38;5;241m.\u001b[39mvalue_counts()[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m neighbour \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m,stop_range):\n\u001b[0;32m---> 19\u001b[0m     x_train_new, y_train_new \u001b[38;5;241m=\u001b[39m \u001b[43msampling_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneighbour\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m     model \u001b[38;5;241m=\u001b[39m train_all_models(x_train_new, y_train_new, model_name)\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;66;03m# Generate predictions\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[10], line 102\u001b[0m, in \u001b[0;36msampling_method\u001b[0;34m(method_name, neighbour, x_train, y_train)\u001b[0m\n\u001b[1;32m     98\u001b[0m     x_train_new, y_train_new \u001b[38;5;241m=\u001b[39m smote_method(x_train,y_train,neighbour)\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m method_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124madasyn\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;66;03m# Apply ADASYN\u001b[39;00m\n\u001b[0;32m--> 102\u001b[0m     x_train_new, y_train_new \u001b[38;5;241m=\u001b[39m \u001b[43madasyn_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mneighbour\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m method_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mborderline_smote\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;66;03m# Apply Borderline SMOTE\u001b[39;00m\n\u001b[1;32m    106\u001b[0m     x_train_new, y_train_new \u001b[38;5;241m=\u001b[39m borderline_smote_method(x_train,y_train,neighbour)\n",
      "Cell \u001b[0;32mIn[10], line 10\u001b[0m, in \u001b[0;36madasyn_method\u001b[0;34m(x, y, neighbour)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madasyn_method\u001b[39m(x,y,neighbour):\n\u001b[1;32m      9\u001b[0m     adap_synt \u001b[38;5;241m=\u001b[39m ADASYN(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, n_neighbors\u001b[38;5;241m=\u001b[39mneighbour)\n\u001b[0;32m---> 10\u001b[0m     x_new, y_new \u001b[38;5;241m=\u001b[39m \u001b[43madap_synt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_resample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x_new, y_new\n",
      "File \u001b[0;32m~/anaconda3/envs/aml/lib/python3.8/site-packages/imblearn/base.py:203\u001b[0m, in \u001b[0;36mBaseSampler.fit_resample\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;124;03m\"\"\"Resample the dataset.\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \n\u001b[1;32m    184\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;124;03m    The corresponding label of `X_resampled`.\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m--> 203\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_resample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/aml/lib/python3.8/site-packages/imblearn/base.py:88\u001b[0m, in \u001b[0;36mSamplerMixin.fit_resample\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     82\u001b[0m X, y, binarize_y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_X_y(X, y)\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampling_strategy_ \u001b[38;5;241m=\u001b[39m check_sampling_strategy(\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampling_strategy, y, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampling_type\n\u001b[1;32m     86\u001b[0m )\n\u001b[0;32m---> 88\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_resample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m y_ \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     91\u001b[0m     label_binarize(output[\u001b[38;5;241m1\u001b[39m], classes\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39munique(y)) \u001b[38;5;28;01mif\u001b[39;00m binarize_y \u001b[38;5;28;01melse\u001b[39;00m output[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     92\u001b[0m )\n\u001b[1;32m     94\u001b[0m X_, y_ \u001b[38;5;241m=\u001b[39m arrays_transformer\u001b[38;5;241m.\u001b[39mtransform(output[\u001b[38;5;241m0\u001b[39m], y_)\n",
      "File \u001b[0;32m~/anaconda3/envs/aml/lib/python3.8/site-packages/imblearn/over_sampling/_adasyn.py:176\u001b[0m, in \u001b[0;36mADASYN._fit_resample\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    173\u001b[0m X_class \u001b[38;5;241m=\u001b[39m _safe_indexing(X, target_class_indices)\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnn_\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[0;32m--> 176\u001b[0m nns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkneighbors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_distance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m[:, \u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m    177\u001b[0m \u001b[38;5;66;03m# The ratio is computed using a one-vs-rest manner. Using majority\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;66;03m# in multi-class would lead to slightly different results at the\u001b[39;00m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;66;03m# cost of introducing a new parameter.\u001b[39;00m\n\u001b[1;32m    180\u001b[0m n_neighbors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnn_\u001b[38;5;241m.\u001b[39mn_neighbors \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/aml/lib/python3.8/site-packages/sklearn/neighbors/_base.py:879\u001b[0m, in \u001b[0;36mKNeighborsMixin.kneighbors\u001b[0;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[1;32m    873\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m issparse(X):\n\u001b[1;32m    874\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    875\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m does not work with sparse matrices. Densify the data, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    876\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor set algorithm=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbrute\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    877\u001b[0m             \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_method\n\u001b[1;32m    878\u001b[0m         )\n\u001b[0;32m--> 879\u001b[0m     chunked_results \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthreads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_tree_query_parallel_helper\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tree\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_neighbors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_distance\u001b[49m\n\u001b[1;32m    882\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    883\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgen_even_slices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    884\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    885\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    886\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minternal: _fit_method not recognized\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/aml/lib/python3.8/site-packages/joblib/parallel.py:1048\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1039\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1040\u001b[0m     \u001b[38;5;66;03m# Only set self._iterating to True if at least a batch\u001b[39;00m\n\u001b[1;32m   1041\u001b[0m     \u001b[38;5;66;03m# was dispatched. In particular this covers the edge\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1045\u001b[0m     \u001b[38;5;66;03m# was very quick and its callback already dispatched all the\u001b[39;00m\n\u001b[1;32m   1046\u001b[0m     \u001b[38;5;66;03m# remaining jobs.\u001b[39;00m\n\u001b[1;32m   1047\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 1048\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_one_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1049\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1051\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n",
      "File \u001b[0;32m~/anaconda3/envs/aml/lib/python3.8/site-packages/joblib/parallel.py:864\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    862\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    863\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 864\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    865\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/aml/lib/python3.8/site-packages/joblib/parallel.py:782\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    780\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    781\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[0;32m--> 782\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[1;32m    784\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[1;32m    785\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[1;32m    786\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[1;32m    787\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[0;32m~/anaconda3/envs/aml/lib/python3.8/site-packages/joblib/_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mImmediateResult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[1;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[0;32m~/anaconda3/envs/aml/lib/python3.8/site-packages/joblib/_parallel_backends.py:572\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[1;32m    571\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[0;32m--> 572\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/aml/lib/python3.8/site-packages/joblib/parallel.py:263\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 263\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    264\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/anaconda3/envs/aml/lib/python3.8/site-packages/joblib/parallel.py:263\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 263\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    264\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/anaconda3/envs/aml/lib/python3.8/site-packages/sklearn/utils/fixes.py:117\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig):\n\u001b[0;32m--> 117\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/aml/lib/python3.8/site-packages/sklearn/neighbors/_base.py:685\u001b[0m, in \u001b[0;36m_tree_query_parallel_helper\u001b[0;34m(tree, *args, **kwargs)\u001b[0m\n\u001b[1;32m    679\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_tree_query_parallel_helper\u001b[39m(tree, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    680\u001b[0m     \u001b[38;5;124;03m\"\"\"Helper for the Parallel calls in KNeighborsMixin.kneighbors.\u001b[39;00m\n\u001b[1;32m    681\u001b[0m \n\u001b[1;32m    682\u001b[0m \u001b[38;5;124;03m    The Cython method tree.query is not directly picklable by cloudpickle\u001b[39;00m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;124;03m    under PyPy.\u001b[39;00m\n\u001b[1;32m    684\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 685\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for model_name in model_list:\n",
    "    metrics_data = []\n",
    "    print(model_name.upper())\n",
    "    \n",
    "    for mth in methods_:\n",
    "        if mth == 'kmeans_smote':\n",
    "            # No cluster are formed beyond this value of k\n",
    "            stop_range = 339\n",
    "        \n",
    "        elif mth == 'smotenc':\n",
    "            # RAM issue\n",
    "            stop_range = 150\n",
    "        \n",
    "        else:\n",
    "            stop_range = y_train.value_counts()[1]\n",
    "\n",
    "\n",
    "        for neighbour in range(1,stop_range):\n",
    "            x_train_new, y_train_new = sampling_method(mth, neighbour, x_train, y_train)\n",
    "\n",
    "            model = train_all_models(x_train_new, y_train_new, model_name)\n",
    "\n",
    "            # Generate predictions\n",
    "            y_pred_train, y_pred_test, y_pred_test_proba = prediction(model, x_train_new, x_test)\n",
    "\n",
    "            # Calculate evaluation metrics\n",
    "            model_evaluation = Evaluation(y_train_new, y_test, y_pred_train, y_pred_test, y_pred_test_proba)\n",
    "\n",
    "            all_metrics,_ = model_evaluation.main()\n",
    "            all_metrics.insert(0, model_name)\n",
    "            all_metrics.insert(1, mth)\n",
    "            all_metrics.insert(2, neighbour)\n",
    "    \n",
    "            if neighbour%1000 == 0 or neighbour == 1:\n",
    "                print(\"{:<6} :: Train Acc: {:<14} :: Test Acc: {}\".format(neighbour, all_metrics[3], all_metrics[4]))\n",
    "#                 print('.', end='')\n",
    "            metrics_data.append(all_metrics)\n",
    "\n",
    "    metrics_df = pd.DataFrame(metrics_data, columns=metrics_data_columns)\n",
    "    metrics_df.to_csv(os.path.join(evaluation_files_loc, f'bos_{model_name}_eval_num_data.csv'), header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f9ff7fd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>sampling_method</th>\n",
       "      <th>k_neighbour</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>precision_0</th>\n",
       "      <th>recall_0</th>\n",
       "      <th>f1_0</th>\n",
       "      <th>precision_1</th>\n",
       "      <th>recall_1</th>\n",
       "      <th>f1_1</th>\n",
       "      <th>ks_stat</th>\n",
       "      <th>p_value</th>\n",
       "      <th>tp</th>\n",
       "      <th>tn</th>\n",
       "      <th>fp</th>\n",
       "      <th>fn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gaussian_naive_bayes</td>\n",
       "      <td>smote</td>\n",
       "      <td>1</td>\n",
       "      <td>0.716641</td>\n",
       "      <td>0.6830</td>\n",
       "      <td>0.699481</td>\n",
       "      <td>0.906012</td>\n",
       "      <td>0.671689</td>\n",
       "      <td>0.771449</td>\n",
       "      <td>0.361416</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.482871</td>\n",
       "      <td>0.411698</td>\n",
       "      <td>0.0</td>\n",
       "      <td>296</td>\n",
       "      <td>1070</td>\n",
       "      <td>523</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gaussian_naive_bayes</td>\n",
       "      <td>smote</td>\n",
       "      <td>2</td>\n",
       "      <td>0.710126</td>\n",
       "      <td>0.6795</td>\n",
       "      <td>0.692710</td>\n",
       "      <td>0.902027</td>\n",
       "      <td>0.670433</td>\n",
       "      <td>0.769175</td>\n",
       "      <td>0.356618</td>\n",
       "      <td>0.714988</td>\n",
       "      <td>0.475879</td>\n",
       "      <td>0.403646</td>\n",
       "      <td>0.0</td>\n",
       "      <td>291</td>\n",
       "      <td>1068</td>\n",
       "      <td>525</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gaussian_naive_bayes</td>\n",
       "      <td>smote</td>\n",
       "      <td>3</td>\n",
       "      <td>0.711224</td>\n",
       "      <td>0.6770</td>\n",
       "      <td>0.691141</td>\n",
       "      <td>0.901612</td>\n",
       "      <td>0.667294</td>\n",
       "      <td>0.766955</td>\n",
       "      <td>0.354446</td>\n",
       "      <td>0.714988</td>\n",
       "      <td>0.473941</td>\n",
       "      <td>0.403322</td>\n",
       "      <td>0.0</td>\n",
       "      <td>291</td>\n",
       "      <td>1063</td>\n",
       "      <td>530</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gaussian_naive_bayes</td>\n",
       "      <td>smote</td>\n",
       "      <td>4</td>\n",
       "      <td>0.711146</td>\n",
       "      <td>0.6765</td>\n",
       "      <td>0.691742</td>\n",
       "      <td>0.902211</td>\n",
       "      <td>0.666039</td>\n",
       "      <td>0.766342</td>\n",
       "      <td>0.354369</td>\n",
       "      <td>0.717445</td>\n",
       "      <td>0.474411</td>\n",
       "      <td>0.406049</td>\n",
       "      <td>0.0</td>\n",
       "      <td>292</td>\n",
       "      <td>1061</td>\n",
       "      <td>532</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gaussian_naive_bayes</td>\n",
       "      <td>smote</td>\n",
       "      <td>5</td>\n",
       "      <td>0.710126</td>\n",
       "      <td>0.6745</td>\n",
       "      <td>0.690486</td>\n",
       "      <td>0.901877</td>\n",
       "      <td>0.663528</td>\n",
       "      <td>0.764557</td>\n",
       "      <td>0.352657</td>\n",
       "      <td>0.717445</td>\n",
       "      <td>0.472874</td>\n",
       "      <td>0.409457</td>\n",
       "      <td>0.0</td>\n",
       "      <td>292</td>\n",
       "      <td>1057</td>\n",
       "      <td>536</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1962</th>\n",
       "      <td>gaussian_naive_bayes</td>\n",
       "      <td>adasyn</td>\n",
       "      <td>334</td>\n",
       "      <td>0.710628</td>\n",
       "      <td>0.6700</td>\n",
       "      <td>0.688576</td>\n",
       "      <td>0.901809</td>\n",
       "      <td>0.657250</td>\n",
       "      <td>0.760349</td>\n",
       "      <td>0.349225</td>\n",
       "      <td>0.719902</td>\n",
       "      <td>0.470305</td>\n",
       "      <td>0.391738</td>\n",
       "      <td>0.0</td>\n",
       "      <td>293</td>\n",
       "      <td>1047</td>\n",
       "      <td>546</td>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1963</th>\n",
       "      <td>gaussian_naive_bayes</td>\n",
       "      <td>adasyn</td>\n",
       "      <td>335</td>\n",
       "      <td>0.712258</td>\n",
       "      <td>0.6750</td>\n",
       "      <td>0.692629</td>\n",
       "      <td>0.903336</td>\n",
       "      <td>0.662900</td>\n",
       "      <td>0.764663</td>\n",
       "      <td>0.353791</td>\n",
       "      <td>0.722359</td>\n",
       "      <td>0.474960</td>\n",
       "      <td>0.394533</td>\n",
       "      <td>0.0</td>\n",
       "      <td>294</td>\n",
       "      <td>1056</td>\n",
       "      <td>537</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1964</th>\n",
       "      <td>gaussian_naive_bayes</td>\n",
       "      <td>adasyn</td>\n",
       "      <td>336</td>\n",
       "      <td>0.711404</td>\n",
       "      <td>0.6730</td>\n",
       "      <td>0.690459</td>\n",
       "      <td>0.902314</td>\n",
       "      <td>0.661017</td>\n",
       "      <td>0.763043</td>\n",
       "      <td>0.351741</td>\n",
       "      <td>0.719902</td>\n",
       "      <td>0.472581</td>\n",
       "      <td>0.396416</td>\n",
       "      <td>0.0</td>\n",
       "      <td>293</td>\n",
       "      <td>1053</td>\n",
       "      <td>540</td>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1965</th>\n",
       "      <td>gaussian_naive_bayes</td>\n",
       "      <td>adasyn</td>\n",
       "      <td>337</td>\n",
       "      <td>0.706827</td>\n",
       "      <td>0.6725</td>\n",
       "      <td>0.690145</td>\n",
       "      <td>0.902230</td>\n",
       "      <td>0.660389</td>\n",
       "      <td>0.762595</td>\n",
       "      <td>0.351319</td>\n",
       "      <td>0.719902</td>\n",
       "      <td>0.472200</td>\n",
       "      <td>0.391057</td>\n",
       "      <td>0.0</td>\n",
       "      <td>293</td>\n",
       "      <td>1052</td>\n",
       "      <td>541</td>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1966</th>\n",
       "      <td>gaussian_naive_bayes</td>\n",
       "      <td>adasyn</td>\n",
       "      <td>338</td>\n",
       "      <td>0.711404</td>\n",
       "      <td>0.6700</td>\n",
       "      <td>0.688576</td>\n",
       "      <td>0.901809</td>\n",
       "      <td>0.657250</td>\n",
       "      <td>0.760349</td>\n",
       "      <td>0.349225</td>\n",
       "      <td>0.719902</td>\n",
       "      <td>0.470305</td>\n",
       "      <td>0.392812</td>\n",
       "      <td>0.0</td>\n",
       "      <td>293</td>\n",
       "      <td>1047</td>\n",
       "      <td>546</td>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1967 rows  18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     model sampling_method  k_neighbour  train_accuracy  \\\n",
       "0     gaussian_naive_bayes           smote            1        0.716641   \n",
       "1     gaussian_naive_bayes           smote            2        0.710126   \n",
       "2     gaussian_naive_bayes           smote            3        0.711224   \n",
       "3     gaussian_naive_bayes           smote            4        0.711146   \n",
       "4     gaussian_naive_bayes           smote            5        0.710126   \n",
       "...                    ...             ...          ...             ...   \n",
       "1962  gaussian_naive_bayes          adasyn          334        0.710628   \n",
       "1963  gaussian_naive_bayes          adasyn          335        0.712258   \n",
       "1964  gaussian_naive_bayes          adasyn          336        0.711404   \n",
       "1965  gaussian_naive_bayes          adasyn          337        0.706827   \n",
       "1966  gaussian_naive_bayes          adasyn          338        0.711404   \n",
       "\n",
       "      test_accuracy   roc_auc  precision_0  recall_0      f1_0  precision_1  \\\n",
       "0            0.6830  0.699481     0.906012  0.671689  0.771449     0.361416   \n",
       "1            0.6795  0.692710     0.902027  0.670433  0.769175     0.356618   \n",
       "2            0.6770  0.691141     0.901612  0.667294  0.766955     0.354446   \n",
       "3            0.6765  0.691742     0.902211  0.666039  0.766342     0.354369   \n",
       "4            0.6745  0.690486     0.901877  0.663528  0.764557     0.352657   \n",
       "...             ...       ...          ...       ...       ...          ...   \n",
       "1962         0.6700  0.688576     0.901809  0.657250  0.760349     0.349225   \n",
       "1963         0.6750  0.692629     0.903336  0.662900  0.764663     0.353791   \n",
       "1964         0.6730  0.690459     0.902314  0.661017  0.763043     0.351741   \n",
       "1965         0.6725  0.690145     0.902230  0.660389  0.762595     0.351319   \n",
       "1966         0.6700  0.688576     0.901809  0.657250  0.760349     0.349225   \n",
       "\n",
       "      recall_1      f1_1   ks_stat  p_value   tp    tn   fp   fn  \n",
       "0     0.727273  0.482871  0.411698      0.0  296  1070  523  111  \n",
       "1     0.714988  0.475879  0.403646      0.0  291  1068  525  116  \n",
       "2     0.714988  0.473941  0.403322      0.0  291  1063  530  116  \n",
       "3     0.717445  0.474411  0.406049      0.0  292  1061  532  115  \n",
       "4     0.717445  0.472874  0.409457      0.0  292  1057  536  115  \n",
       "...        ...       ...       ...      ...  ...   ...  ...  ...  \n",
       "1962  0.719902  0.470305  0.391738      0.0  293  1047  546  114  \n",
       "1963  0.722359  0.474960  0.394533      0.0  294  1056  537  113  \n",
       "1964  0.719902  0.472581  0.396416      0.0  293  1053  540  114  \n",
       "1965  0.719902  0.472200  0.391057      0.0  293  1052  541  114  \n",
       "1966  0.719902  0.470305  0.392812      0.0  293  1047  546  114  \n",
       "\n",
       "[1967 rows x 18 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_df = pd.DataFrame(metrics_data, columns=metrics_data_columns)\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d052887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " SMOTE\n",
      "...................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................."
     ]
    }
   ],
   "source": [
    "   \n",
    "# for mth in methods_:\n",
    "#     print('\\n',mth.upper())\n",
    "#     metrics_data = [] \n",
    "#     if mth == 'kmeans_smote':\n",
    "#         # No cluster are formed beyond this value of k\n",
    "#         stop_range = 339\n",
    "\n",
    "#     elif mth == 'smotenc':\n",
    "#         # RAM issue\n",
    "#         stop_range = 150\n",
    "\n",
    "#     else:\n",
    "#         stop_range = y_train.value_counts()[1]\n",
    "\n",
    "\n",
    "#     for neighbour in range(1,stop_range):\n",
    "#         x_train_new, y_train_new = sampling_method(mth, neighbour)\n",
    "        \n",
    "#         for model_name in model_list:\n",
    "# #             print(model_name.upper())\n",
    "\n",
    "#             model = train_all_models(x_train_new, y_train_new, model_name)\n",
    "\n",
    "#             # Generate predictions\n",
    "#             y_pred_train, y_pred_test, y_pred_test_proba = prediction(model, x_train_new, x_test)\n",
    "\n",
    "#             # Calculate evaluation metrics\n",
    "#             model_evaluation = Evaluation(y_train_new, y_test, y_pred_train, y_pred_test, y_pred_test_proba)\n",
    "\n",
    "#             all_metrics,_ = model_evaluation.main()\n",
    "#             all_metrics.insert(0, model_name)\n",
    "#             all_metrics.insert(1, mth)\n",
    "#             all_metrics.insert(2, neighbour)\n",
    "\n",
    "# #             if neighbour%500 == 0 or neighbour == 1:\n",
    "# #                 print(\"{:<40} :: Train Acc: {:<14} :: Test Acc: {}\".format(mth.upper(), all_metrics[3], all_metrics[4]))\n",
    "\n",
    "#             metrics_data.append(all_metrics)\n",
    "#         print('.',end='')\n",
    "\n",
    "#     metrics_df = pd.DataFrame(metrics_data, columns=metrics_data_columns)\n",
    "#     metrics_df.to_csv(f'./all_methods_models/{mth}_models_evaluation_all.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bbc900",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cc052d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
